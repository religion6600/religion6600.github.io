[
  {
    "objectID": "code/clustering.html",
    "href": "code/clustering.html",
    "title": "Clustering Analysis",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import silhouette_score\n\n\n\n\nCode\ndf = pd.read_csv(\"../data/religion_data_no99.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nHAPPY\nSATIS_A\nSATIS_B\nCHNG_A\nCHNG_B\nCHNG_C\nDIVRELPOP\nDIVRACPOP\nQB2A\nQB2C\n...\nYEARLOCREC\nMOVED\nREG\nPARTY\nIDEO\nHH1REC\nINTFREQ\nINC_SDT1\nGENDER\nFERTREC\n\n\n\n\n0\n1\n2\n2\n2\n3\n2\n2\n3\n2\n2\n...\n4\n1\n1\n1\n1\n3\n2\n2\n2\n1\n\n\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n...\n3\n2\n1\n2\n5\n3\n2\n7\n2\n2\n\n\n2\n2\n2\n3\n3\n1\n1\n1\n1\n1\n2\n...\n4\n1\n1\n3\n3\n2\n1\n7\n2\n0\n\n\n3\n3\n2\n4\n2\n2\n2\n2\n2\n2\n1\n...\n1\n2\n1\n3\n1\n1\n1\n6\n1\n0\n\n\n4\n2\n4\n2\n1\n1\n1\n1\n1\n1\n2\n...\n2\n2\n1\n2\n4\n2\n2\n5\n2\n0\n\n\n\n\n5 rows × 101 columns\n\n\n\n\n\n\n\nCode\n# Reload original DataFrame if it's been modified\ndf = pd.read_csv(\"../data/religion_data_no99.csv\")\n\n# from Best subset selection in mlp.ipynb\nbest_feature_indices = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\n# Convert to list of column names from index positions\nselected_names = [df.columns[i] for i in best_feature_indices if i &lt; len(df.columns)]\nprint(selected_names)\n\n\n['HAPPY', 'DIVRACPOP', 'QB2A', 'QB2D', 'OPENIDEN', 'GOVSIZE1', 'ABRTLGL', 'PAR2CHILD', 'EVOL', 'GUIDE_B', 'GUIDE_D', 'RELTRAD', 'RELPER', 'SPIRPER', 'ATTNDPERRLS', 'ATTNDONRLS', 'MEMB', 'GOD', 'HLL', 'SOUL', 'PRAY', 'GRACE', 'PRAC_A', 'PRAC_B', 'EXP_D', 'EXP_G', 'SPIRACT_C', 'SPIRACT_F', 'RTRT', 'SCIMPACT', 'SPIRWORLD2', 'SECBEL2', 'CHIMPREL_A', 'CHIMPREL_B', 'CHATTEND', 'GTHGHT', 'MARITAL', 'RELINST_B', 'RELINST_D', 'RELINST_F', 'SCPRY2', 'RELDISP', 'CHRNAT', 'BIRTHDECADE', 'RACECMB', 'AFROHISP', 'E2', 'USGEN', 'YEARLOCREC', 'MOVED']\n\n\n\n\nCode\nfeatures = [\n    \"HAPPY\",\n    \"DIVRACPOP\",\n    \"QB2A\",\n    \"QB2D\",\n    \"OPENIDEN\",\n    \"GOVSIZE1\",\n    \"ABRTLGL\",\n    \"PAR2CHILD\",\n    \"EVOL\",\n    \"GUIDE_B\",\n    \"GUIDE_D\",\n    # \"RELTRAD\",\n    \"RELPER\",\n    \"SPIRPER\",\n    \"ATTNDPERRLS\",\n    \"ATTNDONRLS\",\n    \"MEMB\",\n    \"GOD\",\n    \"HLL\",\n    \"SOUL\",\n    \"PRAY\",\n    \"GRACE\",\n    \"PRAC_A\",\n    \"PRAC_B\",\n    \"EXP_D\",\n    \"EXP_G\",\n    \"SPIRACT_C\",\n    \"SPIRACT_F\",\n    \"RTRT\",\n    \"SCIMPACT\",\n    \"SPIRWORLD2\",\n    \"SECBEL2\",\n    \"CHIMPREL_A\",\n    \"CHIMPREL_B\",\n    \"CHATTEND\",\n    \"GTHGHT\",\n    \"MARITAL\",\n    \"RELINST_B\",\n    \"RELINST_D\",\n    \"RELINST_F\",\n    \"SCPRY2\",\n    \"RELDISP\",\n    \"CHRNAT\",\n    \"BIRTHDECADE\",\n    \"RACECMB\",\n    \"AFROHISP\",\n    \"E2\",\n    \"USGEN\",\n    \"YEARLOCREC\",\n    \"MOVED\",\n]\n\n\n\n\n\nWe applied KMeans clustering to a set of respondents using a range of non-religious socio-political and demographic features, including views on gender and sexuality, political ideology, education, income, attitudes toward science and government, and more. The goal was to determine whether individuals naturally group into distinct sociocultural profiles — and to assess how those profiles correspond (if at all) with religious tradition (RELTRAD).\n\n\nOptimalk with elbow method is k=6.\n\n\nCode\nX = df[features].dropna()  # Drop missing values\ndf_clean = df.loc[X.index]  # Preserve indices\n\n# One-hot encode relevant categorical features\ncategorical_cols = [\"MARITAL\", \"BIRTHDECADE\", \"RACECMB\", \"AFROHISP\", \"E2\"]\nX = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n# Elbow Method to choose k\nwcss = []\nfor i in range(2, 10):\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(2, 10), wcss, marker=\"o\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\n\n\n\n\n\n\n\n\nSilhouette Score demonstrates that optimal k=2, but there are more than two religions so this may be limiting - we won’t be able to map religions directly but we can possibly see traditionalist religions vs progressive.\n\n\n\n\n\nCode\nsilhouette_scores = []\nK_range = range(2, 10)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    score = silhouette_score(X_scaled, labels)\n    silhouette_scores.append(score)\n\nplt.figure(figsize=(8, 5))\nplt.plot(K_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score for different k')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.xticks(K_range)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, random_state=42)\ndf_clean['cluster'] = kmeans.fit_predict(X_scaled)\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_clean['pca1'] = X_pca[:, 0]\ndf_clean['pca2'] = X_pca[:, 1]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df_clean, x='pca1', y='pca2', hue='cluster', palette='Set2')\nplt.title(\"KMeans Clustering with k=2 (PCA-reduced)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(df_clean['cluster'], df_clean['RELTRAD'])\n\n\n\n\n\n\n\n\nRELTRAD\n1100\n1200\n1300\n10000\n20000\n30000\n40001\n50000\n60000\n70000\n80000\n100000\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n432\n1170\n73\n1352\n28\n48\n2\n412\n47\n173\n98\n6692\n\n\n1\n4435\n1876\n737\n2722\n334\n84\n32\n109\n120\n59\n66\n663\n\n\n\n\n\n\n\nCluster profiles by feature (Standardized Average)\n\n\nCode\ncluster_profiles = df_clean.groupby('cluster')[features].mean().T\ncluster_profiles.columns = [f'Cluster {i}' for i in cluster_profiles.columns]\ncluster_profiles.plot(kind='bar', figsize=(16, 6), colormap='Set2')\nplt.title(\"Cluster Feature Averages (k=2)\")\nplt.ylabel(\"Mean value (standardized scale)\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCluster 0: Religiously Committed & Socially Conservative\n\nHigher religiosity overall: Greater belief in God, prayer frequency, belief in heaven/hell/soul, and spiritual practices.\nHigher scores on religious engagement: Includes membership, religious experiences (SPIRPER, RELPER), and institutional religion variables like ATTNDPERRLS, CHATTEND, and RELINST_*.\nStronger traditional values: More conservative responses on family, moral values (QB2D), and government size (GOVSIZE1).\nHigher activity in spiritual life: Notable scores on ritual participation (PRAC_A, PRAC_B, SPIRACT_*, RTRT).\nSlightly older, more racially homogeneous (e.g., lower AFROHISP, RACECMB) and more stable (e.g., YEARLOCREC, MOVED).\nLikely to represent a culturally conservative, institutionally religious profile.\n\nCluster 1: Spiritual/Exploratory & Less Traditional\n\nLess committed to institutional religion: Lower average on formal practices and beliefs (e.g., prayer, GOD, HELL, MEMB).\nMore individualistic or exploratory spiritual profile: Still some belief in spiritual realms but weaker scores on structured practices.\nMore supportive of progressive norms: Slightly higher openness (OPENIDEN), environmental concern (SCIMPACT), and diversity tolerance (DIVRACPOP).\nYounger or more mobile: Slightly lower stability markers like YEARLOCREC and MOVED.\nMore religiously disaffiliated: Possibly includes unaffiliated, non-religious, or “spiritual but not religious” individuals.\nSlightly more racially/ethnically diverse, higher score on AFROHISP, RACECMB.\n\n*RELTRAD Distribution per Cluster\n\n\nCode\nreltrad_labels = {\n    1100: \"Evangelical Protestant\",\n    1200: \"Mainline Protestant\",\n    1300: \"Historically Black Protestant\",\n    10000: \"Catholic\",\n    20000: \"Mormon\",\n    30000: \"Orthodox Christian\",\n    40001: \"Jehovah's Witness\",\n    40002: \"Other Christian\",\n    50000: \"Jewish\",\n    60000: \"Muslim\",\n    70000: \"Buddhist\",\n    80000: \"Hindu\",\n    90001: \"Other World Religions\",\n    90002: \"Other Faiths\",\n    100000: \"Religiously Unaffiliated\",\n    900000: \"Don't know/refused\"\n}\n\n\ndf_clean['RELTRAD_label'] = df_clean['RELTRAD'].map(reltrad_labels)\nct = pd.crosstab(df_clean['cluster'], df_clean['RELTRAD_label'], normalize='index')\n\nct.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='tab20')\nplt.title(\"RELTRAD Distribution by Cluster (k=2)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 0 is dominated by the Religiously Unaffiliated — over half of the group identifies this way.\n\nThis aligns with a spiritual but less institutionally religious profile.\nIt also includes a moderate share of Mainline Protestants and Catholics, reflecting a more progressive or moderate religious expression.\n\nCluster 1 is more religiously committed overall.\n\nIt’s heavily made up of Evangelical Protestants, Catholics, and Historically Black Protestants.\nThese groups are more closely tied to institutional religious identity and traditional social values.\n\nWhile all major traditions (e.g., Protestant, Catholic) span both clusters, the subgroup differences matter:\n\nEvangelicals and Black Protestants are much more prominent in Cluster 1.\n\nMainline Protestants are more present in Cluster 0, suggesting a more moderate or progressive religious identity.\n\nSo, while religion correlates with worldview, it’s not the tradition label alone — it’s the type of affiliation and how it’s practiced that drives clustering.\n\nThis reveals intra-religious diversity: people in the same tradition don’t always share the same values or sociopolitical orientation.\n\nKey Insight: The strong presence of the unaffiliated in Cluster 0 — paired with ideological diversity within religious groups — highlights the divide between formal religiosity and personal spirituality or cultural religion.\n\n*CURREL Distribution per Cluster\n\n\nCode\n# Map CURREL codes to readable labels\ncurrel_labels = {\n    1000: \"Protestant\",\n    10000: \"Catholic\",\n    20000: \"Mormon\",\n    30000: \"Orthodox Christian\",\n    40001: \"Jehovah's Witness\",\n    40002: \"Other Christian\",\n    50000: \"Jewish\",\n    60000: \"Muslim\",\n    70000: \"Buddhist\",\n    80000: \"Hindu\",\n    90001: \"Other world religions\",\n    90002: \"Other faiths\",\n    100000: \"Religiously unaffiliated\",\n    900000: \"Refused/uninterpretable\",\n}\n\n# Apply label mapping\ndf_clean[\"CURREL_label\"] = df_clean[\"CURREL\"].map(currel_labels)\n\n# Optional: exclude ambiguous categories\nexcluded_currel = [\n    \"Refused/uninterpretable\",\n    \"Other Christian\",\n    \"Other world religions\",\n    \"Other faiths\",\n]\ndf_clean_currel = df_clean[~df_clean[\"CURREL_label\"].isin(excluded_currel)]\n\n# Create normalized crosstab\nct_currel = pd.crosstab(\n    df_clean_currel[\"cluster\"], df_clean_currel[\"CURREL_label\"], normalize=\"index\"\n)\n\n# Plot\nct_currel.plot(kind=\"bar\", stacked=True, figsize=(14, 7), colormap=\"tab20\")\nplt.title(\"CURREL Distribution by Cluster (k=2)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 0 is dominated by the Religiously Unaffiliated, who make up the majority of this group.\n\nThis tracks with the earlier finding that Cluster 0 reflects a less institutionally religious, more spiritual or secular population.\nThe remaining respondents include Catholics and a small mix of minority traditions (e.g., Buddhist, Jewish), suggesting a more religiously diverse but less traditionally religious cluster overall.\n\nCluster 1 is strongly Protestant — over half of the group identifies as Protestant when using the broad CURREL label.\n\nIt also includes a sizable share of Catholics, making this a clearly more traditionally religious group.\nMinority traditions are represented here as well, but in smaller numbers.\n\nCompared to RELTRAD, CURREL’s broader categorization masks important differences within Protestantism.\n\nWhere RELTRAD distinguished between Evangelical, Mainline, and Black Protestant subgroups, CURREL collapses them — which exaggerates the religious vs. secular divide.\n\nKey Insight: Using CURREL simplifies the religious landscape into a sharper contrast between Protestant (Cluster 1) and Unaffiliated (Cluster 0) —\nbut at the cost of losing the ideological diversity within those large traditions that RELTRAD reveals.\n\n\n\n\n\n\nNow we will use k=5 to capture more nuanced sociopoltiical differences and possible capture differences between the two subgroups.\n\n\nCode\nkmeans_5 = KMeans(n_clusters=5, random_state=42)\ndf_clean[\"cluster_5\"] = kmeans_5.fit_predict(X_scaled)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_clean[\"pca1\"] = X_pca[:, 0]\ndf_clean[\"pca2\"] = X_pca[:, 1]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df_clean, x=\"pca1\", y=\"pca2\", hue=\"cluster_5\", palette=\"Set2\")\nplt.title(\"KMeans Clustering with k=5 (PCA-reduced)\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(df_clean['cluster'], df_clean['CURREL'])\n\n\n\n\n\n\n\n\nCURREL\n1000\n10000\n20000\n30000\n40001\n50000\n60000\n70000\n80000\n100000\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1675\n1352\n28\n48\n2\n412\n47\n173\n98\n6692\n\n\n1\n7048\n2722\n334\n84\n32\n109\n120\n59\n66\n663\n\n\n\n\n\n\n\nCluster profiles by feature (Standardized Average)\n\n\nCode\ncluster_profiles_5 = df_clean.groupby('cluster_5')[features].mean().T\ncluster_profiles_5.columns = [f'Cluster {i}' for i in cluster_profiles_5.columns]\ncluster_profiles_5.plot(kind='bar', figsize=(16, 6), colormap='Set2')\nplt.title(\"Cluster Feature Averages (k=5)\")\nplt.ylabel(\"Mean value (standardized scale)\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCluster 0 (Green-teal)\nThis group tends to be slightly more moderate or mixed overall.\n\nModerate scores on most sociopolitical and religious indicators\n\nAverage levels of belief, practice, and spirituality\n\nSomewhat lower education and income\n\nLess ideologically extreme compared to others\n\nThis cluster may represent a blend of individuals with mixed traditional and secular tendencies, possibly more centrist or disengaged.\nCluster 1 (Blue)\nThis is the most consistently progressive cluster.\n\nHigh support for LGBTQ+ rights, abortion, and diversity\n\nStrong belief in evolution and scientific reasoning (GUIDE_B, GUIDE_D)\n\nPolitically liberal, Democratic-leaning\n\nHigh education and income\n\nLow religiosity and traditional markers\n\nThis group appears to be secular, highly educated, and socially liberal — likely young professionals or cultural progressives.\nCluster 2 (Light Green)\nThe most religiously devout and traditional group.\n\nVery high scores on religious beliefs (GOD, HLL, SOUL, PRAY) and practice\n\nHigh religiosity indicators (MEMB, ATTNDONRLS, CHATTEND)\n\nLower political liberalism\n\nLower belief in science-based reasoning\n\nStrong traditional values around family and gender roles\n\nThis is a classically religious conservative group with strong spiritual commitments and more traditional moral views.\nCluster 3 (Pink)\nCivic-oriented and socially engaged cluster.\n\nHigh political participation (IDEO, PARTY), social concern, and group involvement\n\nStrong on science indicators and pro-social values\n\nModerate to high religiosity, but more practice than strict belief\n\nHigh openness and diversity acceptance\n\nThis group is civically involved and socially inclusive — possibly religious moderates who are socially progressive.\nCluster 4 (Purple-gray)\nA spiritually-inclined but skeptical or questioning cluster.\n\nLower institutional religiosity, but moderate levels of spiritual activity (SPIRACT, SPIRPER)\n\nHigh scores on existential openness (e.g., SOUL, GRACE)\n\nLess trust in organized religion\n\nMore individualistic or exploratory in values\n\nThis may reflect the “spiritual but not religious” demographic — interested in meaning and morality but detached from institutions.\n*RELTRAD Distribution per Cluster\n\n\nCode\n# Optional: drop \"refused\" or unclear categories\nexcluded = [\"Don't know/refused\", \"Other Christian\", \"Other World Religions\", \"Other Faiths\"]\ndf_clean = df_clean[~df_clean['RELTRAD_label'].isin(excluded)]\n\n# RELTRAD by cluster\nct_5 = pd.crosstab(df_clean['cluster_5'], df_clean['RELTRAD_label'], normalize='index')\nct_5.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='tab20')\nplt.title(\"RELTRAD Distribution by Cluster (k=5)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 0: Predominantly made up of Historically Black Protestants, with secondary representation from Evangelical Protestants and Catholics.\n\nThis cluster likely represents a traditionally religious group, potentially with strong communal or institutional religious ties.\nDespite the Evangelical share, it is not dominated by unaffiliated individuals like some other clusters.\n\nCluster 1: Heavily Evangelical Protestant.\n\nThe proportion of Evangelicals is the largest of any cluster, with smaller shares from other traditions.\nThis likely reflects a socially conservative, religiously committed group, in line with Evangelical affiliation.\n\nCluster 2: Overwhelmingly Religiously Unaffiliated.\n\nThis cluster has the highest concentration of unaffiliated individuals, with minimal representation from traditional religious groups.\nIt strongly suggests a secular or spiritually independent profile, potentially linked to non-institutional values and ideologies.\n\nCluster 3: Diverse mix — large shares of Catholics, Mainline Protestants, and Evangelicals.\n\nThis balanced representation points to moderate religiosity or pluralistic orientation.\nIt may reflect those who identify with a religion but are less ideologically extreme or institutionally embedded.\n\nCluster 4: High representation of the Religiously Unaffiliated, but with meaningful portions of Mainline Protestants and Catholics.\n\nThis blend implies a semi-secular group that may combine religious identity with more progressive or individualized belief systems.\n\n\nSummary Insight: - Clusters 1 and 0 lean most strongly toward institutional religiosity, especially with Evangelical and Historically Black Protestant affiliations. - Cluster 2 is the most clearly secular, dominated by the unaffiliated. - Clusters 3 and 4 fall in the middle, combining moderate religious identity with some degree of secularization or pluralism.\nThis distribution further reinforces the idea that religious identity and worldview are not one-to-one — even highly religious groups like Evangelicals appear across clusters, but their dominant presence in one cluster signals ideological clustering within religion itself.\n*CURREL Distribution per Cluster\n\n\nCode\nkmeans_5 = KMeans(n_clusters=5, random_state=42)\ndf_clean['cluster_5'] = kmeans_5.fit_predict(X_scaled)\ndf_clean_currel = df_clean[~df_clean['CURREL_label'].isin(excluded_currel)]\n\n\nct_currel_5 = pd.crosstab(df_clean_currel['cluster_5'], df_clean_currel['CURREL_label'], normalize='index')\n\n# Plot the stacked bar chart\nct_currel_5.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='tab20')\nplt.title(\"CURREL Distribution by Cluster (k=5)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 2 is by far the most secular cluster, with the Religiously Unaffiliated making up nearly the entire group.\n\nThis aligns with the RELTRAD distribution for this cluster, confirming a consistently non-religious identity.\n\nClusters 0 and 3 show a strong Protestant majority, which is much more pronounced under CURREL than in RELTRAD.\n\nThis reflects how collapsing Protestant subgroups (Evangelical, Mainline, Black Protestant) into a single label masks important ideological variation seen in RELTRAD.\nThese clusters also contain a modest share of Catholics and religious minorities.\n\nCluster 1, while also Protestant-heavy, includes a notable presence of Mormon, Muslim, and Catholic identifiers — giving it a more institutionally religious and theologically diverse makeup than others.\nCluster 4 reflects a more mixed composition, with moderate shares of both Protestants and the Unaffiliated, plus some Jewish and Catholic identifiers — suggesting a less clearly polarized religious identity.\n\nKey Insight: - RELTRAD and CURREL tell subtly different stories.\n- CURREL simplifies religious identity into broad categories, exaggerating the Protestant/Unaffiliated split.\n- RELTRAD captures within-group diversity — showing that, for example, not all Protestants cluster the same way.\nSo while “religion” as a single label might feel like a strong predictor, it’s really the granularity of religious identification that matters in understanding sociopolitical alignment."
  },
  {
    "objectID": "code/clustering.html#selecting-relevant-features",
    "href": "code/clustering.html#selecting-relevant-features",
    "title": "Clustering Analysis",
    "section": "",
    "text": "Code\n# Reload original DataFrame if it's been modified\ndf = pd.read_csv(\"../data/religion_data_no99.csv\")\n\n# from Best subset selection in mlp.ipynb\nbest_feature_indices = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\n# Convert to list of column names from index positions\nselected_names = [df.columns[i] for i in best_feature_indices if i &lt; len(df.columns)]\nprint(selected_names)\n\n\n['HAPPY', 'DIVRACPOP', 'QB2A', 'QB2D', 'OPENIDEN', 'GOVSIZE1', 'ABRTLGL', 'PAR2CHILD', 'EVOL', 'GUIDE_B', 'GUIDE_D', 'RELTRAD', 'RELPER', 'SPIRPER', 'ATTNDPERRLS', 'ATTNDONRLS', 'MEMB', 'GOD', 'HLL', 'SOUL', 'PRAY', 'GRACE', 'PRAC_A', 'PRAC_B', 'EXP_D', 'EXP_G', 'SPIRACT_C', 'SPIRACT_F', 'RTRT', 'SCIMPACT', 'SPIRWORLD2', 'SECBEL2', 'CHIMPREL_A', 'CHIMPREL_B', 'CHATTEND', 'GTHGHT', 'MARITAL', 'RELINST_B', 'RELINST_D', 'RELINST_F', 'SCPRY2', 'RELDISP', 'CHRNAT', 'BIRTHDECADE', 'RACECMB', 'AFROHISP', 'E2', 'USGEN', 'YEARLOCREC', 'MOVED']\n\n\n\n\nCode\nfeatures = [\n    \"HAPPY\",\n    \"DIVRACPOP\",\n    \"QB2A\",\n    \"QB2D\",\n    \"OPENIDEN\",\n    \"GOVSIZE1\",\n    \"ABRTLGL\",\n    \"PAR2CHILD\",\n    \"EVOL\",\n    \"GUIDE_B\",\n    \"GUIDE_D\",\n    # \"RELTRAD\",\n    \"RELPER\",\n    \"SPIRPER\",\n    \"ATTNDPERRLS\",\n    \"ATTNDONRLS\",\n    \"MEMB\",\n    \"GOD\",\n    \"HLL\",\n    \"SOUL\",\n    \"PRAY\",\n    \"GRACE\",\n    \"PRAC_A\",\n    \"PRAC_B\",\n    \"EXP_D\",\n    \"EXP_G\",\n    \"SPIRACT_C\",\n    \"SPIRACT_F\",\n    \"RTRT\",\n    \"SCIMPACT\",\n    \"SPIRWORLD2\",\n    \"SECBEL2\",\n    \"CHIMPREL_A\",\n    \"CHIMPREL_B\",\n    \"CHATTEND\",\n    \"GTHGHT\",\n    \"MARITAL\",\n    \"RELINST_B\",\n    \"RELINST_D\",\n    \"RELINST_F\",\n    \"SCPRY2\",\n    \"RELDISP\",\n    \"CHRNAT\",\n    \"BIRTHDECADE\",\n    \"RACECMB\",\n    \"AFROHISP\",\n    \"E2\",\n    \"USGEN\",\n    \"YEARLOCREC\",\n    \"MOVED\",\n]"
  },
  {
    "objectID": "code/clustering.html#k-means",
    "href": "code/clustering.html#k-means",
    "title": "Clustering Analysis",
    "section": "",
    "text": "We applied KMeans clustering to a set of respondents using a range of non-religious socio-political and demographic features, including views on gender and sexuality, political ideology, education, income, attitudes toward science and government, and more. The goal was to determine whether individuals naturally group into distinct sociocultural profiles — and to assess how those profiles correspond (if at all) with religious tradition (RELTRAD).\n\n\nOptimalk with elbow method is k=6.\n\n\nCode\nX = df[features].dropna()  # Drop missing values\ndf_clean = df.loc[X.index]  # Preserve indices\n\n# One-hot encode relevant categorical features\ncategorical_cols = [\"MARITAL\", \"BIRTHDECADE\", \"RACECMB\", \"AFROHISP\", \"E2\"]\nX = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n# Elbow Method to choose k\nwcss = []\nfor i in range(2, 10):\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    kmeans.fit(X_scaled)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(2, 10), wcss, marker=\"o\")\nplt.title(\"Elbow Method for Optimal k\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\n\n\n\n\n\n\n\n\nSilhouette Score demonstrates that optimal k=2, but there are more than two religions so this may be limiting - we won’t be able to map religions directly but we can possibly see traditionalist religions vs progressive.\n\n\n\n\n\nCode\nsilhouette_scores = []\nK_range = range(2, 10)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X_scaled)\n    score = silhouette_score(X_scaled, labels)\n    silhouette_scores.append(score)\n\nplt.figure(figsize=(8, 5))\nplt.plot(K_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score for different k')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.xticks(K_range)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, random_state=42)\ndf_clean['cluster'] = kmeans.fit_predict(X_scaled)\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_clean['pca1'] = X_pca[:, 0]\ndf_clean['pca2'] = X_pca[:, 1]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df_clean, x='pca1', y='pca2', hue='cluster', palette='Set2')\nplt.title(\"KMeans Clustering with k=2 (PCA-reduced)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(df_clean['cluster'], df_clean['RELTRAD'])\n\n\n\n\n\n\n\n\nRELTRAD\n1100\n1200\n1300\n10000\n20000\n30000\n40001\n50000\n60000\n70000\n80000\n100000\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n432\n1170\n73\n1352\n28\n48\n2\n412\n47\n173\n98\n6692\n\n\n1\n4435\n1876\n737\n2722\n334\n84\n32\n109\n120\n59\n66\n663\n\n\n\n\n\n\n\nCluster profiles by feature (Standardized Average)\n\n\nCode\ncluster_profiles = df_clean.groupby('cluster')[features].mean().T\ncluster_profiles.columns = [f'Cluster {i}' for i in cluster_profiles.columns]\ncluster_profiles.plot(kind='bar', figsize=(16, 6), colormap='Set2')\nplt.title(\"Cluster Feature Averages (k=2)\")\nplt.ylabel(\"Mean value (standardized scale)\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCluster 0: Religiously Committed & Socially Conservative\n\nHigher religiosity overall: Greater belief in God, prayer frequency, belief in heaven/hell/soul, and spiritual practices.\nHigher scores on religious engagement: Includes membership, religious experiences (SPIRPER, RELPER), and institutional religion variables like ATTNDPERRLS, CHATTEND, and RELINST_*.\nStronger traditional values: More conservative responses on family, moral values (QB2D), and government size (GOVSIZE1).\nHigher activity in spiritual life: Notable scores on ritual participation (PRAC_A, PRAC_B, SPIRACT_*, RTRT).\nSlightly older, more racially homogeneous (e.g., lower AFROHISP, RACECMB) and more stable (e.g., YEARLOCREC, MOVED).\nLikely to represent a culturally conservative, institutionally religious profile.\n\nCluster 1: Spiritual/Exploratory & Less Traditional\n\nLess committed to institutional religion: Lower average on formal practices and beliefs (e.g., prayer, GOD, HELL, MEMB).\nMore individualistic or exploratory spiritual profile: Still some belief in spiritual realms but weaker scores on structured practices.\nMore supportive of progressive norms: Slightly higher openness (OPENIDEN), environmental concern (SCIMPACT), and diversity tolerance (DIVRACPOP).\nYounger or more mobile: Slightly lower stability markers like YEARLOCREC and MOVED.\nMore religiously disaffiliated: Possibly includes unaffiliated, non-religious, or “spiritual but not religious” individuals.\nSlightly more racially/ethnically diverse, higher score on AFROHISP, RACECMB.\n\n*RELTRAD Distribution per Cluster\n\n\nCode\nreltrad_labels = {\n    1100: \"Evangelical Protestant\",\n    1200: \"Mainline Protestant\",\n    1300: \"Historically Black Protestant\",\n    10000: \"Catholic\",\n    20000: \"Mormon\",\n    30000: \"Orthodox Christian\",\n    40001: \"Jehovah's Witness\",\n    40002: \"Other Christian\",\n    50000: \"Jewish\",\n    60000: \"Muslim\",\n    70000: \"Buddhist\",\n    80000: \"Hindu\",\n    90001: \"Other World Religions\",\n    90002: \"Other Faiths\",\n    100000: \"Religiously Unaffiliated\",\n    900000: \"Don't know/refused\"\n}\n\n\ndf_clean['RELTRAD_label'] = df_clean['RELTRAD'].map(reltrad_labels)\nct = pd.crosstab(df_clean['cluster'], df_clean['RELTRAD_label'], normalize='index')\n\nct.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='tab20')\nplt.title(\"RELTRAD Distribution by Cluster (k=2)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 0 is dominated by the Religiously Unaffiliated — over half of the group identifies this way.\n\nThis aligns with a spiritual but less institutionally religious profile.\nIt also includes a moderate share of Mainline Protestants and Catholics, reflecting a more progressive or moderate religious expression.\n\nCluster 1 is more religiously committed overall.\n\nIt’s heavily made up of Evangelical Protestants, Catholics, and Historically Black Protestants.\nThese groups are more closely tied to institutional religious identity and traditional social values.\n\nWhile all major traditions (e.g., Protestant, Catholic) span both clusters, the subgroup differences matter:\n\nEvangelicals and Black Protestants are much more prominent in Cluster 1.\n\nMainline Protestants are more present in Cluster 0, suggesting a more moderate or progressive religious identity.\n\nSo, while religion correlates with worldview, it’s not the tradition label alone — it’s the type of affiliation and how it’s practiced that drives clustering.\n\nThis reveals intra-religious diversity: people in the same tradition don’t always share the same values or sociopolitical orientation.\n\nKey Insight: The strong presence of the unaffiliated in Cluster 0 — paired with ideological diversity within religious groups — highlights the divide between formal religiosity and personal spirituality or cultural religion.\n\n*CURREL Distribution per Cluster\n\n\nCode\n# Map CURREL codes to readable labels\ncurrel_labels = {\n    1000: \"Protestant\",\n    10000: \"Catholic\",\n    20000: \"Mormon\",\n    30000: \"Orthodox Christian\",\n    40001: \"Jehovah's Witness\",\n    40002: \"Other Christian\",\n    50000: \"Jewish\",\n    60000: \"Muslim\",\n    70000: \"Buddhist\",\n    80000: \"Hindu\",\n    90001: \"Other world religions\",\n    90002: \"Other faiths\",\n    100000: \"Religiously unaffiliated\",\n    900000: \"Refused/uninterpretable\",\n}\n\n# Apply label mapping\ndf_clean[\"CURREL_label\"] = df_clean[\"CURREL\"].map(currel_labels)\n\n# Optional: exclude ambiguous categories\nexcluded_currel = [\n    \"Refused/uninterpretable\",\n    \"Other Christian\",\n    \"Other world religions\",\n    \"Other faiths\",\n]\ndf_clean_currel = df_clean[~df_clean[\"CURREL_label\"].isin(excluded_currel)]\n\n# Create normalized crosstab\nct_currel = pd.crosstab(\n    df_clean_currel[\"cluster\"], df_clean_currel[\"CURREL_label\"], normalize=\"index\"\n)\n\n# Plot\nct_currel.plot(kind=\"bar\", stacked=True, figsize=(14, 7), colormap=\"tab20\")\nplt.title(\"CURREL Distribution by Cluster (k=2)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 0 is dominated by the Religiously Unaffiliated, who make up the majority of this group.\n\nThis tracks with the earlier finding that Cluster 0 reflects a less institutionally religious, more spiritual or secular population.\nThe remaining respondents include Catholics and a small mix of minority traditions (e.g., Buddhist, Jewish), suggesting a more religiously diverse but less traditionally religious cluster overall.\n\nCluster 1 is strongly Protestant — over half of the group identifies as Protestant when using the broad CURREL label.\n\nIt also includes a sizable share of Catholics, making this a clearly more traditionally religious group.\nMinority traditions are represented here as well, but in smaller numbers.\n\nCompared to RELTRAD, CURREL’s broader categorization masks important differences within Protestantism.\n\nWhere RELTRAD distinguished between Evangelical, Mainline, and Black Protestant subgroups, CURREL collapses them — which exaggerates the religious vs. secular divide.\n\nKey Insight: Using CURREL simplifies the religious landscape into a sharper contrast between Protestant (Cluster 1) and Unaffiliated (Cluster 0) —\nbut at the cost of losing the ideological diversity within those large traditions that RELTRAD reveals."
  },
  {
    "objectID": "code/clustering.html#k-5",
    "href": "code/clustering.html#k-5",
    "title": "Clustering Analysis",
    "section": "",
    "text": "Now we will use k=5 to capture more nuanced sociopoltiical differences and possible capture differences between the two subgroups.\n\n\nCode\nkmeans_5 = KMeans(n_clusters=5, random_state=42)\ndf_clean[\"cluster_5\"] = kmeans_5.fit_predict(X_scaled)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_clean[\"pca1\"] = X_pca[:, 0]\ndf_clean[\"pca2\"] = X_pca[:, 1]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df_clean, x=\"pca1\", y=\"pca2\", hue=\"cluster_5\", palette=\"Set2\")\nplt.title(\"KMeans Clustering with k=5 (PCA-reduced)\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(df_clean['cluster'], df_clean['CURREL'])\n\n\n\n\n\n\n\n\nCURREL\n1000\n10000\n20000\n30000\n40001\n50000\n60000\n70000\n80000\n100000\n\n\ncluster\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n1675\n1352\n28\n48\n2\n412\n47\n173\n98\n6692\n\n\n1\n7048\n2722\n334\n84\n32\n109\n120\n59\n66\n663\n\n\n\n\n\n\n\nCluster profiles by feature (Standardized Average)\n\n\nCode\ncluster_profiles_5 = df_clean.groupby('cluster_5')[features].mean().T\ncluster_profiles_5.columns = [f'Cluster {i}' for i in cluster_profiles_5.columns]\ncluster_profiles_5.plot(kind='bar', figsize=(16, 6), colormap='Set2')\nplt.title(\"Cluster Feature Averages (k=5)\")\nplt.ylabel(\"Mean value (standardized scale)\")\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nCluster 0 (Green-teal)\nThis group tends to be slightly more moderate or mixed overall.\n\nModerate scores on most sociopolitical and religious indicators\n\nAverage levels of belief, practice, and spirituality\n\nSomewhat lower education and income\n\nLess ideologically extreme compared to others\n\nThis cluster may represent a blend of individuals with mixed traditional and secular tendencies, possibly more centrist or disengaged.\nCluster 1 (Blue)\nThis is the most consistently progressive cluster.\n\nHigh support for LGBTQ+ rights, abortion, and diversity\n\nStrong belief in evolution and scientific reasoning (GUIDE_B, GUIDE_D)\n\nPolitically liberal, Democratic-leaning\n\nHigh education and income\n\nLow religiosity and traditional markers\n\nThis group appears to be secular, highly educated, and socially liberal — likely young professionals or cultural progressives.\nCluster 2 (Light Green)\nThe most religiously devout and traditional group.\n\nVery high scores on religious beliefs (GOD, HLL, SOUL, PRAY) and practice\n\nHigh religiosity indicators (MEMB, ATTNDONRLS, CHATTEND)\n\nLower political liberalism\n\nLower belief in science-based reasoning\n\nStrong traditional values around family and gender roles\n\nThis is a classically religious conservative group with strong spiritual commitments and more traditional moral views.\nCluster 3 (Pink)\nCivic-oriented and socially engaged cluster.\n\nHigh political participation (IDEO, PARTY), social concern, and group involvement\n\nStrong on science indicators and pro-social values\n\nModerate to high religiosity, but more practice than strict belief\n\nHigh openness and diversity acceptance\n\nThis group is civically involved and socially inclusive — possibly religious moderates who are socially progressive.\nCluster 4 (Purple-gray)\nA spiritually-inclined but skeptical or questioning cluster.\n\nLower institutional religiosity, but moderate levels of spiritual activity (SPIRACT, SPIRPER)\n\nHigh scores on existential openness (e.g., SOUL, GRACE)\n\nLess trust in organized religion\n\nMore individualistic or exploratory in values\n\nThis may reflect the “spiritual but not religious” demographic — interested in meaning and morality but detached from institutions.\n*RELTRAD Distribution per Cluster\n\n\nCode\n# Optional: drop \"refused\" or unclear categories\nexcluded = [\"Don't know/refused\", \"Other Christian\", \"Other World Religions\", \"Other Faiths\"]\ndf_clean = df_clean[~df_clean['RELTRAD_label'].isin(excluded)]\n\n# RELTRAD by cluster\nct_5 = pd.crosstab(df_clean['cluster_5'], df_clean['RELTRAD_label'], normalize='index')\nct_5.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='tab20')\nplt.title(\"RELTRAD Distribution by Cluster (k=5)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 0: Predominantly made up of Historically Black Protestants, with secondary representation from Evangelical Protestants and Catholics.\n\nThis cluster likely represents a traditionally religious group, potentially with strong communal or institutional religious ties.\nDespite the Evangelical share, it is not dominated by unaffiliated individuals like some other clusters.\n\nCluster 1: Heavily Evangelical Protestant.\n\nThe proportion of Evangelicals is the largest of any cluster, with smaller shares from other traditions.\nThis likely reflects a socially conservative, religiously committed group, in line with Evangelical affiliation.\n\nCluster 2: Overwhelmingly Religiously Unaffiliated.\n\nThis cluster has the highest concentration of unaffiliated individuals, with minimal representation from traditional religious groups.\nIt strongly suggests a secular or spiritually independent profile, potentially linked to non-institutional values and ideologies.\n\nCluster 3: Diverse mix — large shares of Catholics, Mainline Protestants, and Evangelicals.\n\nThis balanced representation points to moderate religiosity or pluralistic orientation.\nIt may reflect those who identify with a religion but are less ideologically extreme or institutionally embedded.\n\nCluster 4: High representation of the Religiously Unaffiliated, but with meaningful portions of Mainline Protestants and Catholics.\n\nThis blend implies a semi-secular group that may combine religious identity with more progressive or individualized belief systems.\n\n\nSummary Insight: - Clusters 1 and 0 lean most strongly toward institutional religiosity, especially with Evangelical and Historically Black Protestant affiliations. - Cluster 2 is the most clearly secular, dominated by the unaffiliated. - Clusters 3 and 4 fall in the middle, combining moderate religious identity with some degree of secularization or pluralism.\nThis distribution further reinforces the idea that religious identity and worldview are not one-to-one — even highly religious groups like Evangelicals appear across clusters, but their dominant presence in one cluster signals ideological clustering within religion itself.\n*CURREL Distribution per Cluster\n\n\nCode\nkmeans_5 = KMeans(n_clusters=5, random_state=42)\ndf_clean['cluster_5'] = kmeans_5.fit_predict(X_scaled)\ndf_clean_currel = df_clean[~df_clean['CURREL_label'].isin(excluded_currel)]\n\n\nct_currel_5 = pd.crosstab(df_clean_currel['cluster_5'], df_clean_currel['CURREL_label'], normalize='index')\n\n# Plot the stacked bar chart\nct_currel_5.plot(kind='bar', stacked=True, figsize=(14, 7), colormap='tab20')\nplt.title(\"CURREL Distribution by Cluster (k=5)\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Proportion\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster 2 is by far the most secular cluster, with the Religiously Unaffiliated making up nearly the entire group.\n\nThis aligns with the RELTRAD distribution for this cluster, confirming a consistently non-religious identity.\n\nClusters 0 and 3 show a strong Protestant majority, which is much more pronounced under CURREL than in RELTRAD.\n\nThis reflects how collapsing Protestant subgroups (Evangelical, Mainline, Black Protestant) into a single label masks important ideological variation seen in RELTRAD.\nThese clusters also contain a modest share of Catholics and religious minorities.\n\nCluster 1, while also Protestant-heavy, includes a notable presence of Mormon, Muslim, and Catholic identifiers — giving it a more institutionally religious and theologically diverse makeup than others.\nCluster 4 reflects a more mixed composition, with moderate shares of both Protestants and the Unaffiliated, plus some Jewish and Catholic identifiers — suggesting a less clearly polarized religious identity.\n\nKey Insight: - RELTRAD and CURREL tell subtly different stories.\n- CURREL simplifies religious identity into broad categories, exaggerating the Protestant/Unaffiliated split.\n- RELTRAD captures within-group diversity — showing that, for example, not all Protestants cluster the same way.\nSo while “religion” as a single label might feel like a strong predictor, it’s really the granularity of religious identification that matters in understanding sociopolitical alignment."
  },
  {
    "objectID": "code/clustering.html#political",
    "href": "code/clustering.html#political",
    "title": "Clustering Analysis",
    "section": "Political",
    "text": "Political\n\n\nCode\npolitical_features = [\"GOVSIZE1\", \"IDEO\", \"PARTY\", \"POORASSIST\"]\nX = df[political_features].dropna()\ndf_subset = df.loc[X.index]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Silhouette to find best k\nsil_scores = []\nK_range = range(2, 10)\nfor k in K_range:\n    labels = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)\n    sil_scores.append(silhouette_score(X_scaled, labels))\n\n# Final clustering\nbest_k = K_range[sil_scores.index(max(sil_scores))]\ncluster_labels = KMeans(n_clusters=best_k, random_state=42).fit_predict(X_scaled)\ndf_subset[\"cluster\"] = cluster_labels\n\n# PCA scatter\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_subset[\"pca1\"], df_subset[\"pca2\"] = X_pca[:, 0], X_pca[:, 1]\n\nplt.figure(figsize=(7, 6))\nsns.scatterplot(data=df_subset, x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"Set2\")\nplt.title(f\"PCA Clustering - Political Features (k={best_k})\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n# Cluster feature averages\ncluster_means = df_subset.groupby(\"cluster\")[political_features].mean().T\ncluster_means.columns = [f\"Cluster {i}\" for i in cluster_means.columns]\ncluster_means.plot(kind=\"bar\", figsize=(10, 5), colormap=\"Set2\")\nplt.title(\"Cluster Feature Averages - Political\")\nplt.ylabel(\"Standardized Mean\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# RELTRAD distribution\ndf_subset[\"RELTRAD_label\"] = df_subset[\"RELTRAD\"].map(reltrad_labels)\npd.crosstab(df_subset[\"cluster\"], df_subset[\"RELTRAD_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"RELTRAD by Cluster - Political Features\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n# CURREL distribution\ndf_subset[\"CURREL_label\"] = df_subset[\"CURREL\"].map(currel_labels)\ndf_subset_currel = df_subset[~df_subset[\"CURREL_label\"].isin([\n    \"Other Christian\", \"Other world religions\", \"Other faiths\", \"Refused/uninterpretable\"\n])]\npd.crosstab(df_subset_currel[\"cluster\"], df_subset_currel[\"CURREL_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"CURREL by Cluster - Political Features\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nPolitical Cluster Interpretation (k=4)\n\nCluster 0: Politically moderate-progressive. Mid-to-high on government support for the poor and more liberal (IDEO, PARTY), but slightly lower on trust in large government (GOVSIZE1).\nCluster 1: More conservative. Scores lower on liberal ideology, party identification, and assistance programs. Tends toward traditionalist values.\nCluster 2: Highly liberal-progressive cluster. Very high scores on political ideology (IDEO) and party affiliation, strong support for government aid.\nCluster 3: Politically disengaged or centrist. Lowest scores across the board — could reflect apolitical or ideologically neutral individuals.\n\nRELTRAD & CURREL Distributions:\n\nCluster 2 (liberal) has the largest share of religiously unaffiliated.\nCluster 1 (conservative) is dominated by Evangelical Protestants.\nCluster 0 and 3 are more mixed, with Mainline Protestants and Catholics appearing across both.\nCURREL confirms this: Protestants are concentrated in Cluster 1, while Cluster 2 shows more unaffiliated.\n\nKey Insight: Political identity is tightly tied to religious affiliation in this dataset — Evangelicals cluster together conservatively, while the unaffiliated and Mainline Protestants show more liberal leanings."
  },
  {
    "objectID": "code/clustering.html#religious-practice",
    "href": "code/clustering.html#religious-practice",
    "title": "Clustering Analysis",
    "section": "Religious Practice",
    "text": "Religious Practice\n\n\nCode\npractice_features = [\n    \"ATTNDPERRLS\", \"ATTNDONRLS\", \"PRAY\", \"MEMB\", \"CHATTEND\", \n    \"RELINST_B\", \"RELINST_D\", \"RELINST_F\", \"PRAC_A\", \"PRAC_B\", \"GRACE\"\n]\n\nX = df[practice_features].dropna()\ndf_subset = df.loc[X.index]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Select best k using silhouette score (no plot)\nsil_scores = []\nK_range = range(2, 10)\nfor k in K_range:\n    labels = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)\n    sil_scores.append(silhouette_score(X_scaled, labels))\n\nbest_k = K_range[sil_scores.index(max(sil_scores))]\ncluster_labels = KMeans(n_clusters=best_k, random_state=42).fit_predict(X_scaled)\ndf_subset[\"cluster\"] = cluster_labels\n\n# PCA scatter\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_subset[\"pca1\"], df_subset[\"pca2\"] = X_pca[:, 0], X_pca[:, 1]\n\nplt.figure(figsize=(7, 6))\nsns.scatterplot(data=df_subset, x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"Set2\")\nplt.title(f\"PCA Clustering - Religious Practice (k={best_k})\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n# Cluster feature averages\ncluster_means = df_subset.groupby(\"cluster\")[practice_features].mean().T\ncluster_means.columns = [f\"Cluster {i}\" for i in cluster_means.columns]\ncluster_means.plot(kind=\"bar\", figsize=(10, 5), colormap=\"Set2\")\nplt.title(\"Cluster Feature Averages - Religious Practice\")\nplt.ylabel(\"Standardized Mean\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# RELTRAD bar\ndf_subset[\"RELTRAD_label\"] = df_subset[\"RELTRAD\"].map(reltrad_labels)\npd.crosstab(df_subset[\"cluster\"], df_subset[\"RELTRAD_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"RELTRAD by Cluster - Religious Practice\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n# CURREL bar\ndf_subset[\"CURREL_label\"] = df_subset[\"CURREL\"].map(currel_labels)\ndf_subset_currel = df_subset[~df_subset[\"CURREL_label\"].isin([\n    \"Other Christian\", \"Other world religions\", \"Other faiths\", \"Refused/uninterpretable\"\n])]\npd.crosstab(df_subset_currel[\"cluster\"], df_subset_currel[\"CURREL_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"CURREL by Cluster - Religious Practice\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\nReligious Practice Clustering (k=2)\n\nCluster 0 represents individuals with high levels of religious practice.\n\nThese respondents report frequent attendance at religious services (ATTNDPERRLS, ATTNDONRLS) and higher frequency of prayer (PRAY).\nThey also report higher membership in religious organizations (MEMB) and greater participation in religious rituals or expressions (PRAC_A, PRAC_B, GRACE).\n\nCluster 1 consists of individuals with generally lower levels of formal religious participation and practice.\n\nScores across all variables are consistently lower, suggesting less institutional involvement and personal religious activity.\n\n\nRELTRAD distribution: - Cluster 0 includes a large share of the Religiously Unaffiliated, along with Catholics and Mainline Protestants. - Cluster 1 is strongly dominated by Evangelical Protestants, with additional presence from Historically Black Protestants and Catholics. - The divide aligns closely with levels of religious participation — more frequent participation tracks with Evangelical identity.\nCURREL distribution: - Cluster 0 is more secular overall — Religiously Unaffiliated make up the largest single group. - Cluster 1 is overwhelmingly Protestant, suggesting that current affiliation with a religious tradition correlates strongly with higher levels of religious engagement.\nThese clusters show a clear division between institutionally religious individuals (Cluster 1) and those who are less religiously active or unaffiliated (Cluster 0)."
  },
  {
    "objectID": "code/clustering.html#spiritual-practice",
    "href": "code/clustering.html#spiritual-practice",
    "title": "Clustering Analysis",
    "section": "Spiritual Practice",
    "text": "Spiritual Practice\n\n\nCode\nspiritual_features = [\n    \"PRAY\", \"GRACE\", \"PRAC_A\", \"PRAC_B\", \"EXP_D\", \"EXP_G\", \"SPIRACT_C\",\n    \"SPIRACT_F\", \"RTRT\", \"SCIMPACT\", \"SPIRWORLD2\", \"SECBEL2\"\n]\n\nX = df[spiritual_features].dropna()\ndf_subset = df.loc[X.index]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Best k by silhouette (no plot shown)\nsil_scores = []\nK_range = range(2, 10)\nfor k in K_range:\n    labels = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)\n    sil_scores.append(silhouette_score(X_scaled, labels))\n\nbest_k = K_range[sil_scores.index(max(sil_scores))]\ncluster_labels = KMeans(n_clusters=best_k, random_state=42).fit_predict(X_scaled)\ndf_subset[\"cluster\"] = cluster_labels\n\n# PCA scatter\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_subset[\"pca1\"], df_subset[\"pca2\"] = X_pca[:, 0], X_pca[:, 1]\n\nplt.figure(figsize=(7, 6))\nsns.scatterplot(data=df_subset, x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"Set2\")\nplt.title(f\"PCA Clustering - Spiritual Practice (k={best_k})\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n# Cluster means plot\ncluster_means = df_subset.groupby(\"cluster\")[spiritual_features].mean().T\ncluster_means.columns = [f\"Cluster {i}\" for i in cluster_means.columns]\ncluster_means.plot(kind=\"bar\", figsize=(12, 5), colormap=\"Set2\")\nplt.title(\"Cluster Feature Averages - Spiritual Practice\")\nplt.ylabel(\"Standardized Mean\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# RELTRAD distribution\ndf_subset[\"RELTRAD_label\"] = df_subset[\"RELTRAD\"].map(reltrad_labels)\npd.crosstab(df_subset[\"cluster\"], df_subset[\"RELTRAD_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"RELTRAD by Cluster - Spiritual Practice\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n# CURREL distribution\ndf_subset[\"CURREL_label\"] = df_subset[\"CURREL\"].map(currel_labels)\ndf_subset_currel = df_subset[~df_subset[\"CURREL_label\"].isin([\n    \"Other Christian\", \"Other world religions\", \"Other faiths\", \"Refused/uninterpretable\"\n])]\npd.crosstab(df_subset_currel[\"cluster\"], df_subset_currel[\"CURREL_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"CURREL by Cluster - Spiritual Practice\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\nCluster 0\n- Demonstrates high engagement in spiritual activities:\n- Frequent prayer (PRAY), grace before meals (GRACE), and both formal and informal religious practice (PRAC_A, PRAC_B)\n- Strong spiritual experiences and activities (EXP_D, EXP_G, SPIRACT_C, SPIRACT_F)\n- Elevated retreat attendance (RTRT)\n- Surprisingly, this spiritually active group is dominated by the religiously unaffiliated — especially in both RELTRAD and CURREL. - That is, many in this cluster do not identify with a formal religion, but still engage deeply in spiritual behavior.\nCluster 1\n- Displays lower but still present levels of spiritual activity:\n- Less ritual participation and fewer experiential indicators\n- Compositionally, this cluster is made up of more traditionally affiliated individuals — especially Evangelical Protestants and Catholics, as shown in both religion variables.\nThis clustering flips common assumptions: the most spiritually active group is not the most traditionally affiliated. In fact, those who identify as religiously unaffiliated dominate the high-engagement cluster — similar to what was observed in religious practice clustering, where Cluster 0 also showed both high practice levels and high unaffiliated rates.\nBehavioral measures of spirituality (like prayer or retreat attendance) are not tightly bound to religious labels. Many “unaffiliated” individuals are deeply engaged in practice, while some traditionally affiliated groups show more nominal participation.\nThis mirrors the pattern seen in religious practice clustering, reinforcing the idea that spiritual action and religious identity are increasingly decoupled in contemporary contexts."
  },
  {
    "objectID": "code/clustering.html#demographics",
    "href": "code/clustering.html#demographics",
    "title": "Clustering Analysis",
    "section": "Demographics",
    "text": "Demographics\n\n\nCode\ndemo_features = [\n    \"BIRTHDECADE\", \"RACECMB\", \"AFROHISP\", \"E2\", \"USGEN\", \"YEARLOCREC\", \"MOVED\", \"MARITAL\"\n]\n\nX = df[demo_features].dropna()\ndf_subset = df.loc[X.index]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Best k by silhouette (no plot)\nsil_scores = []\nK_range = range(2, 10)\nfor k in K_range:\n    labels = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)\n    sil_scores.append(silhouette_score(X_scaled, labels))\n\nbest_k = K_range[sil_scores.index(max(sil_scores))]\ncluster_labels = KMeans(n_clusters=best_k, random_state=42).fit_predict(X_scaled)\ndf_subset[\"cluster\"] = cluster_labels\n\n# PCA plot\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_subset[\"pca1\"], df_subset[\"pca2\"] = X_pca[:, 0], X_pca[:, 1]\n\nplt.figure(figsize=(7, 6))\nsns.scatterplot(data=df_subset, x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"Set2\")\nplt.title(f\"PCA Clustering - Demographics (k={best_k})\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n# Cluster feature averages\ncluster_means = df_subset.groupby(\"cluster\")[demo_features].mean().T\ncluster_means.columns = [f\"Cluster {i}\" for i in cluster_means.columns]\ncluster_means.plot(kind=\"bar\", figsize=(10, 5), colormap=\"Set2\")\nplt.title(\"Cluster Feature Averages - Demographics\")\nplt.ylabel(\"Standardized Mean\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# RELTRAD bar\ndf_subset[\"RELTRAD_label\"] = df_subset[\"RELTRAD\"].map(reltrad_labels)\npd.crosstab(df_subset[\"cluster\"], df_subset[\"RELTRAD_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"RELTRAD by Cluster - Demographics\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n# CURREL bar\ndf_subset[\"CURREL_label\"] = df_subset[\"CURREL\"].map(currel_labels)\ndf_subset_currel = df_subset[~df_subset[\"CURREL_label\"].isin([\n    \"Other Christian\", \"Other world religions\", \"Other faiths\", \"Refused/uninterpretable\"\n])]\npd.crosstab(df_subset_currel[\"cluster\"], df_subset_currel[\"CURREL_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"CURREL by Cluster - Demographics\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\nDemographic Clusters\nCluster 0 consists of individuals who are generally younger, more racially and ethnically diverse (RACECMB, AFROHISP), and more mobile (MOVED). They are slightly more likely to be unmarried and less rooted in their current community.\nCluster 1, in contrast, is older, more likely to be white, and more often married. They also report more geographic stability and higher likelihood of U.S. nativity (USGEN), indicating a more settled and possibly traditional demographic profile.\nFrom a religious identity perspective:\n\nCluster 0 includes a broad religious mix with higher representation of the religiously unaffiliated, along with notable shares of Mainline Protestants and Evangelicals.\nCluster 1 leans slightly more Catholic and Evangelical, with lower unaffiliated representation compared to Cluster 0.\n\nIn terms of CURREL, this same trend is visible: Cluster 0 leans more secular, while Cluster 1 shows greater Protestant and Catholic affiliation.\nWhile these demographic clusters reflect meaningful variation in age, race, and stability, the religious differences between them are not as stark as those seen in clusters based on religious practice or values. This suggests that demographics alone don’t strongly segment religious worldview, but they may help shape predispositions."
  },
  {
    "objectID": "code/clustering.html#science-worldview",
    "href": "code/clustering.html#science-worldview",
    "title": "Clustering Analysis",
    "section": "Science Worldview",
    "text": "Science Worldview\n\n\nCode\nscience_features = [\n    \"EVOL\", \"GUIDE_B\", \"GUIDE_D\", \"GTHGHT\", \"SCPRY2\", \"RELDISP\"\n]\n\nX = df[science_features].dropna()\ndf_subset = df.loc[X.index]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Best silhouette-based k\nsil_scores = []\nK_range = range(2, 10)\nfor k in K_range:\n    labels = KMeans(n_clusters=k, random_state=42).fit_predict(X_scaled)\n    sil_scores.append(silhouette_score(X_scaled, labels))\n\nbest_k = K_range[sil_scores.index(max(sil_scores))]\ncluster_labels = KMeans(n_clusters=best_k, random_state=42).fit_predict(X_scaled)\ndf_subset[\"cluster\"] = cluster_labels\n\n# PCA plot\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf_subset[\"pca1\"], df_subset[\"pca2\"] = X_pca[:, 0], X_pca[:, 1]\n\nplt.figure(figsize=(7, 6))\nsns.scatterplot(data=df_subset, x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"Set2\")\nplt.title(f\"PCA Clustering - Science & Worldview (k={best_k})\")\nplt.legend(title=\"Cluster\")\nplt.tight_layout()\nplt.show()\n\n# Cluster feature averages\ncluster_means = df_subset.groupby(\"cluster\")[science_features].mean().T\ncluster_means.columns = [f\"Cluster {i}\" for i in cluster_means.columns]\ncluster_means.plot(kind=\"bar\", figsize=(10, 5), colormap=\"Set2\")\nplt.title(\"Cluster Feature Averages - Science & Worldview\")\nplt.ylabel(\"Standardized Mean\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# RELTRAD breakdown\ndf_subset[\"RELTRAD_label\"] = df_subset[\"RELTRAD\"].map(reltrad_labels)\npd.crosstab(df_subset[\"cluster\"], df_subset[\"RELTRAD_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"RELTRAD by Cluster - Science & Worldview\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Religious Tradition\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n# CURREL breakdown\ndf_subset[\"CURREL_label\"] = df_subset[\"CURREL\"].map(currel_labels)\ndf_subset_currel = df_subset[~df_subset[\"CURREL_label\"].isin([\n    \"Other Christian\", \"Other world religions\", \"Other faiths\", \"Refused/uninterpretable\"\n])]\npd.crosstab(df_subset_currel[\"cluster\"], df_subset_currel[\"CURREL_label\"], normalize=\"index\")\\\n  .plot(kind=\"bar\", stacked=True, figsize=(12, 6), colormap=\"tab20\")\nplt.title(\"CURREL by Cluster - Science & Worldview\")\nplt.ylabel(\"Proportion\")\nplt.xlabel(\"Cluster\")\nplt.legend(title=\"Current Religion\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\nScience & Worldview Clustering (k=2)\nCluster differences on science and metaphysical worldview are present but less stark than in domains like religious practice.\n\nCluster 0 shows greater belief in evolution and scientific reasoning (e.g., higher means on EVOL, GUIDE_B, GUIDE_D).\n\nHowever, their scores on items like RELIDISP (religious distrust of science) are relatively neutral, not fully secular.\nCompositionally, this cluster includes more religiously affiliated individuals, particularly Evangelical Protestants and Catholics.\n\nCluster 1, in contrast, expresses stronger spiritual openness and lower traditional religiosity.\n\nHigh scores on GTHGHT (sense of purpose) and SCPRY2 (spiritual practices), combined with lower evolution endorsement, suggest a more spiritual-but-not-scientific orientation.\nReligiously, this cluster is dominated by the Unaffiliated, though there’s also a presence of Mainline Protestants and religious minorities.\n\nAcross both RELTRAD and CURREL:\n\nCluster 1 has a much larger share of the Religiously Unaffiliated, while\nCluster 0 includes more Protestants and Catholics.\n\n\nThese clusters suggest that worldview-based divisions are real but less polarized than in direct measures of belief or religious practice. The relationship between science, purpose, and belief appears more complex — possibly shaped by culture, identity, and education as much as formal religion."
  },
  {
    "objectID": "code/MLP.html",
    "href": "code/MLP.html",
    "title": "Data",
    "section": "",
    "text": "Import the dependencies\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf  \nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras import models\nfrom keras import layers\nfrom keras.datasets import boston_housing\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers\nimport pandas as pd\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n\nCollecting mlxtend\n  Downloading mlxtend-0.23.4-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: scipy&gt;=1.2.1 in /opt/homebrew/lib/python3.11/site-packages (from mlxtend) (1.13.1)\nRequirement already satisfied: numpy&gt;=1.16.2 in /opt/homebrew/lib/python3.11/site-packages (from mlxtend) (1.26.4)\nRequirement already satisfied: pandas&gt;=0.24.2 in /opt/homebrew/lib/python3.11/site-packages (from mlxtend) (2.2.2)\nRequirement already satisfied: scikit-learn&gt;=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from mlxtend) (1.6.1)\nRequirement already satisfied: matplotlib&gt;=3.0.0 in /opt/homebrew/lib/python3.11/site-packages (from mlxtend) (3.9.0)\nRequirement already satisfied: joblib&gt;=0.13.2 in /opt/homebrew/lib/python3.11/site-packages (from mlxtend) (1.4.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (4.53.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (24.2)\nRequirement already satisfied: pillow&gt;=8 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (10.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (3.1.2)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib&gt;=3.0.0-&gt;mlxtend) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas&gt;=0.24.2-&gt;mlxtend) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas&gt;=0.24.2-&gt;mlxtend) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.0.0-&gt;mlxtend) (1.17.0)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn&gt;=1.3.1-&gt;mlxtend) (3.5.0)\nDownloading mlxtend-0.23.4-py3-none-any.whl (1.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 26.3 MB/s eta 0:00:00\nInstalling collected packages: mlxtend\nSuccessfully installed mlxtend-0.23.4\nNote: you may need to restart the kernel to use updated packages.\nImport and split the data\nreligion_data = pd.read_csv('../data/religion_data_no99.csv')\nreligion_data.head()\n\n\n\n\n\n\n\n\nHAPPY\nSATIS_A\nSATIS_B\nCHNG_A\nCHNG_B\nCHNG_C\nDIVRELPOP\nDIVRACPOP\nQB2A\nQB2C\n...\nYEARLOCREC\nMOVED\nREG\nPARTY\nIDEO\nHH1REC\nINTFREQ\nINC_SDT1\nGENDER\nFERTREC\n\n\n\n\n0\n1\n2\n2\n2\n3\n2\n2\n3\n2\n2\n...\n4\n1\n1\n1\n1\n3\n2\n2\n2\n1\n\n\n1\n1\n2\n1\n1\n1\n1\n1\n1\n1\n2\n...\n3\n2\n1\n2\n5\n3\n2\n7\n2\n2\n\n\n2\n2\n2\n3\n3\n1\n1\n1\n1\n1\n2\n...\n4\n1\n1\n3\n3\n2\n1\n7\n2\n0\n\n\n3\n3\n2\n4\n2\n2\n2\n2\n2\n2\n1\n...\n1\n2\n1\n3\n1\n1\n1\n6\n1\n0\n\n\n4\n2\n4\n2\n1\n1\n1\n1\n1\n1\n2\n...\n2\n2\n1\n2\n4\n2\n2\n5\n2\n0\n\n\n\n\n5 rows × 101 columns\ndf_clean = religion_data.drop(columns=['RELTRAD'])\ndf_clean['CURREL'].unique()\n\narray([  1000, 100000,  10000,  30000,  50000,  20000,  40001,  80000,\n        70000,  60000])\nfrom sklearn.model_selection import train_test_split\n\ndf_clean = religion_data.drop(columns=['CURREL'])\n\nX = df_clean.drop(columns=['RELTRAD'])  \ny = df_clean['RELTRAD']               \n\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True)\n\nprint(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"X_val:   {X_val.shape},   y_val:   {y_val.shape}\")\nprint(f\"X_test:  {X_test.shape},  y_test:  {y_test.shape}\")\n\nX_train: (17411, 99), y_train: (17411,)\nX_val:   (2176, 99),   y_val:   (2176,)\nX_test:  (2177, 99),  y_test:  (2177,)"
  },
  {
    "objectID": "code/MLP.html#pre-processing",
    "href": "code/MLP.html#pre-processing",
    "title": "Data",
    "section": "Pre-processing",
    "text": "Pre-processing\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)"
  },
  {
    "objectID": "code/MLP.html#feature-selection",
    "href": "code/MLP.html#feature-selection",
    "title": "Data",
    "section": "Feature Selection",
    "text": "Feature Selection\nStepwise Feature Selection\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nimport numpy as np\n\n\nstepwise_rf = RandomForestClassifier(random_state=42)\n\nstepwise_selector = SFS(\n    stepwise_rf,\n    k_features=50,\n    forward=True,\n    floating=True,\n    scoring='accuracy',\n    cv=StratifiedKFold(n_splits=3),\n    n_jobs=-1\n)\n\nstepwise_selector.fit(X_train, y_train)\n\nstepwise_selected_idx = list(stepwise_selector.k_feature_idx_)\nstepwise_X_train = X_train[:, stepwise_selected_idx]\n\nstepwise_scores = cross_val_score(stepwise_rf, stepwise_X_train, y_train, cv=3)\n\nprint(f\"Stepwise-selected {len(stepwise_selected_idx)} features: Accuracy = {np.mean(stepwise_scores):.4f}\")\n\nStepwise-selected 50 features: Accuracy = 0.6539\n\n\n\nprint(stepwise_X_train)\n\n[[-1.36903393 -0.51445703  1.66367039 ... -1.87491355 -0.39500903\n  -1.21986136]\n [ 0.24239453 -0.51445703 -0.6010806  ... -1.87491355 -0.39500903\n  -0.16392986]\n [ 0.24239453 -0.51445703 -0.6010806  ...  0.53335792 -0.39500903\n   0.89200163]\n ...\n [ 1.85382299 -0.51445703 -0.6010806  ...  0.53335792  2.53158768\n  -1.21986136]\n [ 1.85382299  2.20975566 -0.6010806  ...  0.53335792 -0.39500903\n  -0.16392986]\n [-1.36903393 -0.51445703 -0.6010806  ...  0.53335792 -0.39500903\n  -0.16392986]]\n\n\n\n\nfeature_names = X.columns.tolist()\nselected_feature_names = [feature_names[i] for i in stepwise_selected_idx]\n\n\nstepwise_rf.fit(stepwise_X_train, y_train)\nimportances = stepwise_rf.feature_importances_\n\nindices = np.argsort(importances)[::-1][:5]\ntop_importances = importances[indices]\ntop_feature_names = [selected_feature_names[i] for i in indices]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.barh(range(5), top_importances, color='#3274A1')\nplt.yticks(range(5), top_feature_names)\nplt.xlabel(\"Importance Score\")\nplt.title(\"Top 5 Contributing Features\")\nplt.gca().invert_yaxis()  # Highest at top\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nX_val= X_val[:, selected_features]\nX_test = X_test[:, selected_features]\nX_train = X_train[:, selected_features]"
  },
  {
    "objectID": "code/MLP.html#model-setup",
    "href": "code/MLP.html#model-setup",
    "title": "Data",
    "section": "Model setup",
    "text": "Model setup\nCreate model class. This class allows the finetuning of the optimizer, the activation function, regularization and dropout.\n\n\nclass feed_forward_model:\n    def __init__(self, optimizer, activation_func, num_classes, regularization=None, dropout_rate=0):\n        self.model = models.Sequential()\n        input_shape = (X_train.shape[1],)\n\n        if isinstance(regularization, tuple):\n            reg_type, reg_strength = regularization\n        else:\n            reg_type, reg_strength = regularization, None\n\n        reg_strength = float(reg_strength) \n\n        if reg_type == 'l1':\n            kernel_reg = regularizers.L1(reg_strength)\n        elif reg_type == 'l2':\n            kernel_reg = regularizers.L2(reg_strength)\n        else:\n            kernel_reg = None\n\n        self.model.add(layers.Dense(64, activation=activation_func, input_shape=input_shape, kernel_regularizer=kernel_reg))\n\n        if dropout_rate &gt; 0:\n            self.model.add(layers.Dropout(dropout_rate))\n\n        self.model.add(layers.Dense(64, activation=activation_func))\n\n        if dropout_rate &gt; 0:\n            self.model.add(layers.Dropout(dropout_rate))\n\n        self.model.add(layers.Dense(num_classes, activation='softmax'))\n\n        self.model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n    def fit(self, X_train, y_train, X_val, y_val, X_test=None, y_test=None, **kwargs):\n        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n        history = self.model.fit(X_train, y_train, epochs=80, batch_size=16, validation_data=(X_val, y_val), callbacks=[early_stopping], \n                                 verbose=0,**kwargs)\n    \n        if X_test is not None and y_test is not None:\n            test_loss, test_acc = self.model.evaluate(X_test, y_test, verbose=0)\n            print(f'Test set accuracy: {test_acc:.4f}')\n    \n        return history\n\n\n    def predict(self, x):\n        return self.model.predict(x)\n\n    def plot_history(self, history):\n        loss_values = history.history['loss']\n        val_loss_values = history.history['val_loss']\n        epochs = range(1, len(loss_values) + 1)\n        plt.plot(epochs, loss_values, 'r', label='Training loss')\n        plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n        plt.title('Training and validation loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n\n    def plot_confusion_matrix(self, x, y_true, class_labels):\n        import seaborn as sns\n        from sklearn.metrics import confusion_matrix\n        y_pred = self.predict(x).argmax(axis=1)\n        cm = confusion_matrix(y_true, y_pred)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True\")\n        plt.title(\"Confusion Matrix\")"
  },
  {
    "objectID": "code/MLP.html#tuning",
    "href": "code/MLP.html#tuning",
    "title": "Data",
    "section": "Tuning",
    "text": "Tuning\n\n\n\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_val_encoded = label_encoder.transform(y_val)\ny_test_encoded = label_encoder.transform(y_test)\n\nnum_classes = len(label_encoder.classes_)\nprint(num_classes)\n\n\n12\n\n\n\n\noptimizers = ['rmsprop', 'sgd', 'adam']\nactivation_func = ['relu', 'tanh', 'leaky_relu']\nregularizations = [None, 'l1', 'l2']\ndropout_rates = [0, 0.2, 0.5]\nreg_strengths = [0.001, 0.01, 0.1] \n\nresults = []\n\nfor optimizer in optimizers:\n    for activation in activation_func:\n        for regularization in regularizations:\n            for dropout_rate in dropout_rates:\n               \n                reg_strength_vals = [None] if regularization is None else reg_strengths\n\n                for reg_strength in reg_strength_vals:\n                    print(f\"Training: optimizer={optimizer}, activation={activation}, \"\n                          f\"regularization={regularization}, reg_strength={reg_strength}, \"\n                          f\"dropout_rate={dropout_rate}\")\n\n                    \n                    model = feed_forward_model(optimizer=optimizer,\n                                               activation_func=activation,\n                                               num_classes=num_classes,\n                                               regularization=(regularization, reg_strength),\n                                               dropout_rate=dropout_rate)\n\n                   \n                    history = model.fit(X_train, y_train_encoded,\n                                        X_val, y_val_encoded)\n\n                   \n                    y_val_pred_probs = model.model.predict(X_val)\n                    y_val_pred = y_val_pred_probs.argmax(axis=1)\n\n                    precision, recall, f1, _ = precision_recall_fscore_support(\n                        y_val_encoded, y_val_pred, average='macro', zero_division=0)\n\n                    val_acc = history.history['val_accuracy'][-1]\n\n                    results.append({\n                        'optimizer': optimizer,\n                        'activation': activation,\n                        'regularization': regularization,\n                        'reg_strength': reg_strength,\n                        'dropout_rate': dropout_rate,\n                        'val_accuracy': val_acc,\n                        'val_precision': precision,\n                        'val_recall': recall,\n                        'val_f1': f1\n                    })\n\n\nTraining: optimizer=rmsprop, activation=relu, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 271us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 317us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 253us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 362us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 330us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 279us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 264us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 275us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 301us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 293us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 257us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 257us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 284us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 255us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 260us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 321us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 292us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 276us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 311us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 298us/step\nTraining: optimizer=rmsprop, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 465us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 326us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 318us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 310us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 298us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 274us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 315us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 292us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 288us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 304us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 346us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 299us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l1, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 369us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 339us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 293us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 283us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 397us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 301us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 402us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 285us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 322us/step\nTraining: optimizer=rmsprop, activation=tanh, regularization=l2, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 398us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 294us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 322us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 315us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 346us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 342us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 350us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 318us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 408us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 368us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 420us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l1, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 330us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 291us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 313us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 296us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 310us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 330us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 254us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 363us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 449us/step\nTraining: optimizer=rmsprop, activation=leaky_relu, regularization=l2, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 301us/step\nTraining: optimizer=sgd, activation=relu, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 378us/step\nTraining: optimizer=sgd, activation=relu, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 295us/step\nTraining: optimizer=sgd, activation=relu, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 298us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 298us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 315us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 261us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 353us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 335us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 313us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 276us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 301us/step\nTraining: optimizer=sgd, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 565us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 309us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 323us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 355us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 358us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 333us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 328us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 297us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 416us/step\nTraining: optimizer=sgd, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 520us/step\nTraining: optimizer=sgd, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 409us/step\nTraining: optimizer=sgd, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 315us/step\nTraining: optimizer=sgd, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 375us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 317us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 320us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 285us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 326us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 398us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 304us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 352us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 316us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l1, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 344us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 290us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 339us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 270us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 289us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 317us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 321us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 309us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 340us/step\nTraining: optimizer=sgd, activation=tanh, regularization=l2, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 324us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 322us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 392us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 300us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 308us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 342us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 308us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 320us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 365us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 308us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 308us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l1, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 324us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 335us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 364us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 380us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 346us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 331us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 377us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 407us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 478us/step\nTraining: optimizer=sgd, activation=leaky_relu, regularization=l2, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 383us/step\nTraining: optimizer=adam, activation=relu, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 304us/step\nTraining: optimizer=adam, activation=relu, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 330us/step\nTraining: optimizer=adam, activation=relu, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 1s 311us/step \nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 286us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 336us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 297us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 325us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 319us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 312us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 299us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 463us/step\nTraining: optimizer=adam, activation=relu, regularization=l1, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 319us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 288us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 317us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 302us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 356us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 307us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 288us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.001, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 315us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.01, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 286us/step\nTraining: optimizer=adam, activation=relu, regularization=l2, reg_strength=0.1, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 314us/step\nTraining: optimizer=adam, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 340us/step\nTraining: optimizer=adam, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 274us/step\nTraining: optimizer=adam, activation=tanh, regularization=None, reg_strength=None, dropout_rate=0.5\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 313us/step\nTraining: optimizer=adam, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 300us/step\nTraining: optimizer=adam, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 302us/step\nTraining: optimizer=adam, activation=tanh, regularization=l1, reg_strength=0.1, dropout_rate=0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 297us/step\nTraining: optimizer=adam, activation=tanh, regularization=l1, reg_strength=0.001, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n68/68 ━━━━━━━━━━━━━━━━━━━━ 0s 357us/step\nTraining: optimizer=adam, activation=tanh, regularization=l1, reg_strength=0.01, dropout_rate=0.2\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n2025-04-15 12:30:26.282552: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [50,64] vs. [0]\n     [[{{function_node __inference_one_step_on_data_30543591}}{{node gradient_tape/Abs/mul}}]]\n\n\n\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\nCell In[12], line 32\n     20 print(f\"Training: optimizer={optimizer}, activation={activation}, \"\n     21       f\"regularization={regularization}, reg_strength={reg_strength}, \"\n     22       f\"dropout_rate={dropout_rate}\")\n     25 model = feed_forward_model(optimizer=optimizer,\n     26                            activation_func=activation,\n     27                            num_classes=num_classes,\n     28                            regularization=(regularization, reg_strength),\n     29                            dropout_rate=dropout_rate)\n---&gt; 32 history = model.fit(X_train, y_train_encoded,\n     33                     X_val, y_val_encoded)\n     36 y_val_pred_probs = model.model.predict(X_val)\n     37 y_val_pred = y_val_pred_probs.argmax(axis=1)\n\nCell In[10], line 44, in feed_forward_model.fit(self, X_train, y_train, X_val, y_val, X_test, y_test, **kwargs)\n     42 def fit(self, X_train, y_train, X_val, y_val, X_test=None, y_test=None, **kwargs):\n     43     early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n---&gt; 44     history = self.model.fit(X_train, y_train, \n     45                              epochs=80, \n     46                              batch_size=16,\n     47                              validation_data=(X_val, y_val),\n     48                              callbacks=[early_stopping], \n     49                              verbose=0, \n     50                              **kwargs)\n     52     if X_test is not None and y_test is not None:\n     53         test_loss, test_acc = self.model.evaluate(X_test, y_test, verbose=0)\n\nFile /opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\n    120     # To get the full stack trace, call:\n    121     # `keras.config.disable_traceback_filtering()`\n--&gt; 122     raise e.with_traceback(filtered_tb) from None\n    123 finally:\n    124     del filtered_tb\n\nFile /opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     51 try:\n     52   ctx.ensure_initialized()\n---&gt; 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n     54                                       inputs, attrs, num_outputs)\n     55 except core._NotOkStatusException as e:\n     56   if name is not None:\n\nInvalidArgumentError: Graph execution error:\n\nDetected at node gradient_tape/Abs/mul defined at (most recent call last):\n  File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n\n  File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in &lt;module&gt;\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n\n  File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n\n  File \"/opt/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/86/5f040sy516q4gjdt2zq5d7ph0000gn/T/ipykernel_75539/3604593806.py\", line 32, in &lt;module&gt;\n\n  File \"/var/folders/86/5f040sy516q4gjdt2zq5d7ph0000gn/T/ipykernel_75539/1288269813.py\", line 44, in fit\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 113, in one_step_on_data\n\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 77, in train_step\n\nIncompatible shapes: [50,64] vs. [0]\n     [[{{node gradient_tape/Abs/mul}}]] [Op:__inference_multi_step_on_iterator_30543650]\n\n\n\nResults of grid search hyperparameter tuning best model according to validation accuracy: sgd , tanh , l1 = 0.001, dropout rate = 0.0\n\nresults_df = pd.DataFrame(results)\nresults_df_sorted = results_df.sort_values(by='val_accuracy', ascending=False)\n\n\nprint(results_df_sorted)\n\n    optimizer  activation regularization  reg_strength  dropout_rate  \\\n87        sgd        tanh             l1         0.001           0.0   \n99        sgd        tanh             l2         0.001           0.2   \n43    rmsprop  leaky_relu           None           NaN           0.2   \n45    rmsprop  leaky_relu             l1         0.001           0.0   \n69        sgd        relu             l1         0.001           0.2   \n..        ...         ...            ...           ...           ...   \n116       sgd  leaky_relu             l1         0.100           0.5   \n74        sgd        relu             l1         0.100           0.5   \n95        sgd        tanh             l1         0.100           0.5   \n68        sgd        relu             l1         0.100           0.0   \n71        sgd        relu             l1         0.100           0.2   \n\n     val_accuracy  val_precision  val_recall    val_f1  \n87       0.677390       0.652482    0.460693  0.487791  \n99       0.675551       0.611523    0.457477  0.499240  \n43       0.675551       0.579640    0.428334  0.453324  \n45       0.673713       0.698321    0.510160  0.524500  \n69       0.672794       0.531447    0.417811  0.437573  \n..            ...            ...         ...       ...  \n116      0.502757       0.122940    0.170546  0.137330  \n74       0.495404       0.114549    0.158475  0.116700  \n95       0.480239       0.118425    0.165924  0.129726  \n68       0.478401       0.121525    0.164217  0.139484  \n71       0.437500       0.129204    0.173710  0.139563  \n\n[154 rows x 9 columns]"
  },
  {
    "objectID": "code/MLP.html#final-results",
    "href": "code/MLP.html#final-results",
    "title": "Data",
    "section": "Final results",
    "text": "Final results\nOptimal model\n\nbest_config = results_df.sort_values(by='val_accuracy', ascending=False).iloc[0]\n\noptimizer = best_config['optimizer']\nactivation = best_config['activation']\nregularization = best_config['regularization']\nreg_strength = best_config['reg_strength']\ndropout_rate = best_config['dropout_rate']\n\nprint(\"Using best model configuration:\")\nprint(f\"Activation: {activation}, Optimizer: {optimizer}, Regularization: {regularization}, \"\n      f\"Strength: {reg_strength}, Dropout: {dropout_rate}\")\n\n\nbest_model = feed_forward_model(optimizer=optimizer,\n                                activation_func=activation,\n                                num_classes=num_classes,\n                                regularization=(regularization, reg_strength),\n                                dropout_rate=dropout_rate)\n\nhistory = best_model.fit(X_train, y_train_encoded,\n                         X_val, y_val_encoded,\n                         X_test=X_test, y_test=y_test_encoded)\n\n\ny_pred_probs = best_model.model.predict(X_test)\ny_pred = y_pred_probs.argmax(axis=1)\n\nprint(\"\\nClassification Report on Test Set:\")\nprint(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_.astype(str)))\n\n\ncm = confusion_matrix(y_test_encoded, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nUsing best model configuration:\nActivation: tanh, Optimizer: sgd, Regularization: l1, Strength: 0.001, Dropout: 0.0\n\n\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\nTest set accuracy: 0.6775\n69/69 ━━━━━━━━━━━━━━━━━━━━ 0s 501us/step\n\nClassification Report (Test Set):\n              precision    recall  f1-score   support\n\n        1100       0.64      0.75      0.69       500\n        1200       0.49      0.35      0.41       324\n        1300       0.65      0.75      0.69        91\n       10000       0.62      0.62      0.62       402\n       20000       0.57      0.30      0.39        27\n       30000       0.00      0.00      0.00        18\n       40001       0.00      0.00      0.00         4\n       50000       0.59      0.42      0.49        40\n       60000       0.83      0.36      0.50        14\n       70000       0.33      0.09      0.14        23\n       80000       0.47      0.44      0.45        16\n      100000       0.80      0.88      0.84       718\n\n    accuracy                           0.68      2177\n   macro avg       0.50      0.41      0.44      2177\nweighted avg       0.66      0.68      0.66      2177\n\n\n\n/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))"
  },
  {
    "objectID": "code/QDA.html",
    "href": "code/QDA.html",
    "title": "QDA",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.feature_selection import VarianceThreshold\nfrom imblearn.over_sampling import SMOTE\nimport warnings\n\n# dealing with an SkLearn deprecated warning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Orthodox Christian',\n    40001: 'Jehovahs Witness',\n    40002: 'Other Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Buddhist',\n    80000: 'Hindu',\n    90001: 'Other world Religions',\n    90002: 'Other faiths',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant               8723\nUnaffiliated             7355\nCatholic                 4074\nJewish                    521\nOther faiths              508\nMormon                    362\nBuddhist                  232\nMuslim                    167\nHindu                     164\nOrthodox Christian        132\nOther Christian           117\nOther world Religions      59\nJehovahs Witness           34\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 100)\n(14366,)\n\nVALIDATION\n(3592, 100)\n(3592,)\n\nTEST\n(4490, 100)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nplt.figure(figsize=(12, 6))\nsns.countplot(x=y)\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Class Distribution in CURREL\")\nplt.xlabel(\"CURREL NEW\", fontsize=12)\nplt.ylabel(\"Count\", fontsize=12)\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6795657015590201\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                       precision    recall  f1-score   support\n\n             Buddhist       0.22      0.18      0.20        39\n             Catholic       0.56      0.65      0.60       815\n                Hindu       0.48      0.41      0.44        34\n     Jehovahs Witness       1.00      0.10      0.18        10\n               Jewish       0.23      0.49      0.31       107\n               Mormon       0.18      0.65      0.28        69\n               Muslim       0.53      0.27      0.36        37\n   Orthodox Christian       0.00      0.00      0.00        27\n      Other Christian       0.00      0.00      0.00        26\n         Other faiths       0.14      0.38      0.20        87\nOther world Religions       0.00      0.00      0.00        13\n           Protestant       0.82      0.66      0.73      1777\n         Unaffiliated       0.91      0.81      0.86      1449\n\n             accuracy                           0.67      4490\n            macro avg       0.39      0.35      0.32      4490\n         weighted avg       0.74      0.67      0.70      4490\n\n0.6746102449888641\n-----------------------\nCONFUSION MATRIX:\n[[   7    1    2    0    3    0    2    0    0    6    0    2   16]\n [   4  530    2    0   42   39    0    3    4    7    1  171   12]\n [   5    2   14    0    1    0    3    0    0    0    0    1    8]\n [   0    1    0    1    0    0    0    0    0    0    0    8    0]\n [   0   17    0    0   52    1    0    1    0   10    0    7   19]\n [   0   12    0    0    0   45    0    0    0    0    0   11    1]\n [   3    8    4    0    2    0   10    1    0    0    0    6    3]\n [   1   10    1    0    1    0    1    0    0    0    0   12    1]\n [   0    6    0    0    1    1    0    0    0    8    0    7    3]\n [   2    3    0    0    2    0    0    0    0   33    0    5   42]\n [   1    0    1    0    0    0    1    0    0    4    0    2    4]\n [   2  342    0    0   74  161    1    6   11    7    0 1165    8]\n [   7   21    5    0   46    0    1    1    2  168    1   25 1172]]\n-----------------------\nROC CURVES:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Christian',\n    10000: 'Christian',\n    20000: 'Christian',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Non-Christian',\n    60000: 'Non-Christian',\n    70000: 'Non-Christian',\n    80000: 'Non-Christian',\n    90001: 'Non-Christian',\n    90002: 'Non-Christian',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nChristian        13442\nUnaffiliated      7355\nNon-Christian     1651\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 100)\n(14366,)\n\nVALIDATION\n(3592, 100)\n(3592,)\n\nTEST\n(4490, 100)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.8933741648106904\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n               precision    recall  f1-score   support\n\n    Christian       0.97      0.92      0.95      2724\nNon-Christian       0.38      0.57      0.46       317\n Unaffiliated       0.91      0.89      0.90      1449\n\n     accuracy                           0.89      4490\n    macro avg       0.75      0.80      0.77      4490\n weighted avg       0.91      0.89      0.90      4490\n\n0.889532293986637\n-----------------------\nCONFUSION MATRIX:\n[[2519  175   30]\n [  37  181   99]\n [  36  119 1294]]\n-----------------------\nROC CURVES:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Other Religion',\n    80000: 'Other Religion',\n    90001: 'Other Religion',\n    90002: 'Other Religion',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant        8723\nUnaffiliated      7355\nCatholic          4074\nOther Religion     963\nJewish             521\nMormon             362\nChristian          283\nMuslim             167\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 100)\n(14366,)\n\nVALIDATION\n(3592, 100)\n(3592,)\n\nTEST\n(4490, 100)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6909799554565702\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                precision    recall  f1-score   support\n\n      Catholic       0.56      0.64      0.60       815\n     Christian       0.14      0.16      0.15        63\n        Jewish       0.23      0.50      0.32       107\n        Mormon       0.18      0.65      0.29        69\n        Muslim       0.53      0.22      0.31        37\nOther Religion       0.29      0.48      0.36       173\n    Protestant       0.83      0.65      0.73      1777\n  Unaffiliated       0.91      0.82      0.86      1449\n\n      accuracy                           0.68      4490\n     macro avg       0.46      0.52      0.45      4490\n  weighted avg       0.75      0.68      0.71      4490\n\n0.6841870824053452\n-----------------------\nCONFUSION MATRIX:\n[[ 525   18   43   39    0   13  166   11]\n [  14   10    4    1    1    8   22    3]\n [  18    1   54    1    0    5    7   21]\n [  12    0    0   45    0    0   11    1]\n [   6    2    1    0    8   12    6    2]\n [   5    1    8    0    4   83    7   65]\n [ 341   31   71  159    1    6 1160    8]\n [  19    8   51    0    1  159   24 1187]]\n-----------------------\nROC CURVES:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.feature_selection import VarianceThreshold\nfrom imblearn.over_sampling import SMOTE\nimport warnings\n\n# dealing with an SkLearn deprecated warning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Orthodox Christian',\n    40001: 'Jehovahs Witness',\n    40002: 'Other Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Buddhist',\n    80000: 'Hindu',\n    90001: 'Other world Religions',\n    90002: 'Other faiths',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant               8723\nUnaffiliated             7355\nCatholic                 4074\nJewish                    521\nOther faiths              508\nMormon                    362\nBuddhist                  232\nMuslim                    167\nHindu                     164\nOrthodox Christian        132\nOther Christian           117\nOther world Religions      59\nJehovahs Witness           34\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\nselected_features = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\nX_int = X_int.iloc[:, selected_features]\nprint(X_int.shape)\n\n\n(22448, 50)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 50)\n(14366,)\n\nVALIDATION\n(3592, 50)\n(3592,)\n\nTEST\n(4490, 50)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6205456570155902\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                       precision    recall  f1-score   support\n\n             Buddhist       0.12      0.21      0.15        39\n             Catholic       0.61      0.60      0.60       815\n                Hindu       0.24      0.50      0.32        34\n     Jehovahs Witness       0.50      0.20      0.29        10\n               Jewish       0.27      0.53      0.36       107\n               Mormon       0.10      0.72      0.18        69\n               Muslim       0.44      0.43      0.44        37\n   Orthodox Christian       0.02      0.11      0.04        27\n      Other Christian       0.06      0.19      0.09        26\n         Other faiths       0.12      0.39      0.18        87\nOther world Religions       0.00      0.00      0.00        13\n           Protestant       0.84      0.53      0.65      1777\n         Unaffiliated       0.94      0.75      0.84      1449\n\n             accuracy                           0.60      4490\n            macro avg       0.33      0.40      0.32      4490\n         weighted avg       0.77      0.60      0.66      4490\n\n0.6044543429844098\n-----------------------\nCONFUSION MATRIX:\n[[   8    0    2    0    6    0    4    1    1    8    0    2    7]\n [   6  488    1    0   49   67    2   35   25    1    0  139    2]\n [   4    0   17    0    3    0    1    1    0    1    0    0    7]\n [   0    0    0    2    1    2    0    0    0    0    0    5    0]\n [   6   14    4    0   57    2    1    2    4    5    1    4    7]\n [   2    5    0    0    0   50    0    4    1    1    0    5    1]\n [   1    2    6    0    3    2   16    4    0    0    0    1    2]\n [   1    4    1    0    3    4    0    3    1    0    0    9    1]\n [   5    3    0    0    3    2    0    1    5    4    0    3    0]\n [   6    2    1    0    1    1    3    0    2   34    3    0   34]\n [   2    0    2    0    1    0    1    0    1    3    0    0    3]\n [   3  273    0    2   71  349    3   83   44    0    0  947    2]\n [  25   14   38    0   16    3    5    5    3  236   10    7 1087]]\n-----------------------\nROC CURVES:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Christian',\n    10000: 'Christian',\n    20000: 'Christian',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Non-Christian',\n    60000: 'Non-Christian',\n    70000: 'Non-Christian',\n    80000: 'Non-Christian',\n    90001: 'Non-Christian',\n    90002: 'Non-Christian',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nChristian        13442\nUnaffiliated      7355\nNon-Christian     1651\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\nCode\nselected_features = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\nX_int = X_int.iloc[:, selected_features]\nprint(X_int.shape)\n\n\n(22448, 50)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 50)\n(14366,)\n\nVALIDATION\n(3592, 50)\n(3592,)\n\nTEST\n(4490, 50)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.9106347438752784\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n               precision    recall  f1-score   support\n\n    Christian       0.98      0.94      0.96      2724\nNon-Christian       0.42      0.60      0.49       317\n Unaffiliated       0.93      0.91      0.92      1449\n\n     accuracy                           0.91      4490\n    macro avg       0.77      0.82      0.79      4490\n weighted avg       0.92      0.91      0.91      4490\n\n0.9060133630289532\n-----------------------\nCONFUSION MATRIX:\n[[2565  149   10]\n [  35  191   91]\n [  18  119 1312]]\n-----------------------\nROC CURVES:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Other Religion',\n    80000: 'Other Religion',\n    90001: 'Other Religion',\n    90002: 'Other Religion',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant        8723\nUnaffiliated      7355\nCatholic          4074\nOther Religion     963\nJewish             521\nMormon             362\nChristian          283\nMuslim             167\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\nCode\nselected_features = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\nX_int = X_int.iloc[:, selected_features]\nprint(X_int.shape)\n\n\n(22448, 50)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 50)\n(14366,)\n\nVALIDATION\n(3592, 50)\n(3592,)\n\nTEST\n(4490, 50)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6620267260579065\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                precision    recall  f1-score   support\n\n      Catholic       0.60      0.62      0.61       815\n     Christian       0.12      0.30      0.18        63\n        Jewish       0.26      0.60      0.36       107\n        Mormon       0.10      0.74      0.18        69\n        Muslim       0.40      0.49      0.44        37\nOther Religion       0.27      0.45      0.34       173\n    Protestant       0.85      0.55      0.66      1777\n  Unaffiliated       0.94      0.83      0.88      1449\n\n      accuracy                           0.65      4490\n     macro avg       0.44      0.57      0.46      4490\n  weighted avg       0.77      0.65      0.69      4490\n\n0.6478841870824054\n-----------------------\nCONFUSION MATRIX:\n[[ 502   36   55   71    2    4  142    3]\n [   6   19    7    7    0    8   15    1]\n [  13    6   64    2    2    7    4    9]\n [   6    2    1   51    0    2    6    1]\n [   2    5    3    2   18    4    2    1]\n [   3    4   14    1   14   77    1   59]\n [ 289   71   82  359    3    0  971    2]\n [  14    9   23    4    6  181    5 1207]]\n-----------------------\nROC CURVES:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFull Currel: - overall accuracy: 0.67\n- weighted f-1: 0.70\n\nChristian vs Non vs Unaffaliated: - overall accuracy: 0.89\n- weighted f-1: 0.90\n\nMore Categories: - overall accuracy: 0.68\n- weighted f-1: 0.71\n\nFull Currel w Features: - overall accuracy: 0.60\n- weighted f-1: 0.66\n\nChristian vs Non vs Unaffaliated w Features: - overall accuracy: 0.91\n- weighted f-1: 0.91\n\nMore Categories w Features: - overall accuracy: 0.65\n- weighted f-1: 0.69"
  },
  {
    "objectID": "code/QDA.html#all-currel-categories",
    "href": "code/QDA.html#all-currel-categories",
    "title": "QDA",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.feature_selection import VarianceThreshold\nfrom imblearn.over_sampling import SMOTE\nimport warnings\n\n# dealing with an SkLearn deprecated warning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Orthodox Christian',\n    40001: 'Jehovahs Witness',\n    40002: 'Other Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Buddhist',\n    80000: 'Hindu',\n    90001: 'Other world Religions',\n    90002: 'Other faiths',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant               8723\nUnaffiliated             7355\nCatholic                 4074\nJewish                    521\nOther faiths              508\nMormon                    362\nBuddhist                  232\nMuslim                    167\nHindu                     164\nOrthodox Christian        132\nOther Christian           117\nOther world Religions      59\nJehovahs Witness           34\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 100)\n(14366,)\n\nVALIDATION\n(3592, 100)\n(3592,)\n\nTEST\n(4490, 100)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nplt.figure(figsize=(12, 6))\nsns.countplot(x=y)\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Class Distribution in CURREL\")\nplt.xlabel(\"CURREL NEW\", fontsize=12)\nplt.ylabel(\"Count\", fontsize=12)\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6795657015590201\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                       precision    recall  f1-score   support\n\n             Buddhist       0.22      0.18      0.20        39\n             Catholic       0.56      0.65      0.60       815\n                Hindu       0.48      0.41      0.44        34\n     Jehovahs Witness       1.00      0.10      0.18        10\n               Jewish       0.23      0.49      0.31       107\n               Mormon       0.18      0.65      0.28        69\n               Muslim       0.53      0.27      0.36        37\n   Orthodox Christian       0.00      0.00      0.00        27\n      Other Christian       0.00      0.00      0.00        26\n         Other faiths       0.14      0.38      0.20        87\nOther world Religions       0.00      0.00      0.00        13\n           Protestant       0.82      0.66      0.73      1777\n         Unaffiliated       0.91      0.81      0.86      1449\n\n             accuracy                           0.67      4490\n            macro avg       0.39      0.35      0.32      4490\n         weighted avg       0.74      0.67      0.70      4490\n\n0.6746102449888641\n-----------------------\nCONFUSION MATRIX:\n[[   7    1    2    0    3    0    2    0    0    6    0    2   16]\n [   4  530    2    0   42   39    0    3    4    7    1  171   12]\n [   5    2   14    0    1    0    3    0    0    0    0    1    8]\n [   0    1    0    1    0    0    0    0    0    0    0    8    0]\n [   0   17    0    0   52    1    0    1    0   10    0    7   19]\n [   0   12    0    0    0   45    0    0    0    0    0   11    1]\n [   3    8    4    0    2    0   10    1    0    0    0    6    3]\n [   1   10    1    0    1    0    1    0    0    0    0   12    1]\n [   0    6    0    0    1    1    0    0    0    8    0    7    3]\n [   2    3    0    0    2    0    0    0    0   33    0    5   42]\n [   1    0    1    0    0    0    1    0    0    4    0    2    4]\n [   2  342    0    0   74  161    1    6   11    7    0 1165    8]\n [   7   21    5    0   46    0    1    1    2  168    1   25 1172]]\n-----------------------\nROC CURVES:"
  },
  {
    "objectID": "code/QDA.html#christian-vs-non-christian-vs-unaffiliated",
    "href": "code/QDA.html#christian-vs-non-christian-vs-unaffiliated",
    "title": "QDA",
    "section": "",
    "text": "Code\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Christian',\n    10000: 'Christian',\n    20000: 'Christian',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Non-Christian',\n    60000: 'Non-Christian',\n    70000: 'Non-Christian',\n    80000: 'Non-Christian',\n    90001: 'Non-Christian',\n    90002: 'Non-Christian',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nChristian        13442\nUnaffiliated      7355\nNon-Christian     1651\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 100)\n(14366,)\n\nVALIDATION\n(3592, 100)\n(3592,)\n\nTEST\n(4490, 100)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.8933741648106904\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n               precision    recall  f1-score   support\n\n    Christian       0.97      0.92      0.95      2724\nNon-Christian       0.38      0.57      0.46       317\n Unaffiliated       0.91      0.89      0.90      1449\n\n     accuracy                           0.89      4490\n    macro avg       0.75      0.80      0.77      4490\n weighted avg       0.91      0.89      0.90      4490\n\n0.889532293986637\n-----------------------\nCONFUSION MATRIX:\n[[2519  175   30]\n [  37  181   99]\n [  36  119 1294]]\n-----------------------\nROC CURVES:"
  },
  {
    "objectID": "code/QDA.html#protestant-catholic-mormon-christian-jewish-muslim-other-unaffaliated",
    "href": "code/QDA.html#protestant-catholic-mormon-christian-jewish-muslim-other-unaffaliated",
    "title": "QDA",
    "section": "",
    "text": "Code\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Other Religion',\n    80000: 'Other Religion',\n    90001: 'Other Religion',\n    90002: 'Other Religion',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant        8723\nUnaffiliated      7355\nCatholic          4074\nOther Religion     963\nJewish             521\nMormon             362\nChristian          283\nMuslim             167\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 100)\n(14366,)\n\nVALIDATION\n(3592, 100)\n(3592,)\n\nTEST\n(4490, 100)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6909799554565702\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                precision    recall  f1-score   support\n\n      Catholic       0.56      0.64      0.60       815\n     Christian       0.14      0.16      0.15        63\n        Jewish       0.23      0.50      0.32       107\n        Mormon       0.18      0.65      0.29        69\n        Muslim       0.53      0.22      0.31        37\nOther Religion       0.29      0.48      0.36       173\n    Protestant       0.83      0.65      0.73      1777\n  Unaffiliated       0.91      0.82      0.86      1449\n\n      accuracy                           0.68      4490\n     macro avg       0.46      0.52      0.45      4490\n  weighted avg       0.75      0.68      0.71      4490\n\n0.6841870824053452\n-----------------------\nCONFUSION MATRIX:\n[[ 525   18   43   39    0   13  166   11]\n [  14   10    4    1    1    8   22    3]\n [  18    1   54    1    0    5    7   21]\n [  12    0    0   45    0    0   11    1]\n [   6    2    1    0    8   12    6    2]\n [   5    1    8    0    4   83    7   65]\n [ 341   31   71  159    1    6 1160    8]\n [  19    8   51    0    1  159   24 1187]]\n-----------------------\nROC CURVES:"
  },
  {
    "objectID": "code/QDA.html#using-the-selected-features-for-all-currel",
    "href": "code/QDA.html#using-the-selected-features-for-all-currel",
    "title": "QDA",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, RocCurveDisplay\nfrom sklearn.preprocessing import StandardScaler, label_binarize\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import compute_class_weight\nfrom sklearn.feature_selection import VarianceThreshold\nfrom imblearn.over_sampling import SMOTE\nimport warnings\n\n# dealing with an SkLearn deprecated warning\nwarnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn\")\n\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Orthodox Christian',\n    40001: 'Jehovahs Witness',\n    40002: 'Other Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Buddhist',\n    80000: 'Hindu',\n    90001: 'Other world Religions',\n    90002: 'Other faiths',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant               8723\nUnaffiliated             7355\nCatholic                 4074\nJewish                    521\nOther faiths              508\nMormon                    362\nBuddhist                  232\nMuslim                    167\nHindu                     164\nOrthodox Christian        132\nOther Christian           117\nOther world Religions      59\nJehovahs Witness           34\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\n\n\n\nCode\nselected_features = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\nX_int = X_int.iloc[:, selected_features]\nprint(X_int.shape)\n\n\n(22448, 50)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 50)\n(14366,)\n\nVALIDATION\n(3592, 50)\n(3592,)\n\nTEST\n(4490, 50)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6205456570155902\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                       precision    recall  f1-score   support\n\n             Buddhist       0.12      0.21      0.15        39\n             Catholic       0.61      0.60      0.60       815\n                Hindu       0.24      0.50      0.32        34\n     Jehovahs Witness       0.50      0.20      0.29        10\n               Jewish       0.27      0.53      0.36       107\n               Mormon       0.10      0.72      0.18        69\n               Muslim       0.44      0.43      0.44        37\n   Orthodox Christian       0.02      0.11      0.04        27\n      Other Christian       0.06      0.19      0.09        26\n         Other faiths       0.12      0.39      0.18        87\nOther world Religions       0.00      0.00      0.00        13\n           Protestant       0.84      0.53      0.65      1777\n         Unaffiliated       0.94      0.75      0.84      1449\n\n             accuracy                           0.60      4490\n            macro avg       0.33      0.40      0.32      4490\n         weighted avg       0.77      0.60      0.66      4490\n\n0.6044543429844098\n-----------------------\nCONFUSION MATRIX:\n[[   8    0    2    0    6    0    4    1    1    8    0    2    7]\n [   6  488    1    0   49   67    2   35   25    1    0  139    2]\n [   4    0   17    0    3    0    1    1    0    1    0    0    7]\n [   0    0    0    2    1    2    0    0    0    0    0    5    0]\n [   6   14    4    0   57    2    1    2    4    5    1    4    7]\n [   2    5    0    0    0   50    0    4    1    1    0    5    1]\n [   1    2    6    0    3    2   16    4    0    0    0    1    2]\n [   1    4    1    0    3    4    0    3    1    0    0    9    1]\n [   5    3    0    0    3    2    0    1    5    4    0    3    0]\n [   6    2    1    0    1    1    3    0    2   34    3    0   34]\n [   2    0    2    0    1    0    1    0    1    3    0    0    3]\n [   3  273    0    2   71  349    3   83   44    0    0  947    2]\n [  25   14   38    0   16    3    5    5    3  236   10    7 1087]]\n-----------------------\nROC CURVES:"
  },
  {
    "objectID": "code/QDA.html#using-selected-for-christian-vs-non-christian-vs-unaffiliated",
    "href": "code/QDA.html#using-selected-for-christian-vs-non-christian-vs-unaffiliated",
    "title": "QDA",
    "section": "",
    "text": "Code\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Christian',\n    10000: 'Christian',\n    20000: 'Christian',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Non-Christian',\n    60000: 'Non-Christian',\n    70000: 'Non-Christian',\n    80000: 'Non-Christian',\n    90001: 'Non-Christian',\n    90002: 'Non-Christian',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nChristian        13442\nUnaffiliated      7355\nNon-Christian     1651\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\nCode\nselected_features = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\nX_int = X_int.iloc[:, selected_features]\nprint(X_int.shape)\n\n\n(22448, 50)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 50)\n(14366,)\n\nVALIDATION\n(3592, 50)\n(3592,)\n\nTEST\n(4490, 50)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.9106347438752784\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n               precision    recall  f1-score   support\n\n    Christian       0.98      0.94      0.96      2724\nNon-Christian       0.42      0.60      0.49       317\n Unaffiliated       0.93      0.91      0.92      1449\n\n     accuracy                           0.91      4490\n    macro avg       0.77      0.82      0.79      4490\n weighted avg       0.92      0.91      0.91      4490\n\n0.9060133630289532\n-----------------------\nCONFUSION MATRIX:\n[[2565  149   10]\n [  35  191   91]\n [  18  119 1312]]\n-----------------------\nROC CURVES:"
  },
  {
    "objectID": "code/QDA.html#using-selected-for-protestant-catholic-mormon-christian-jewish-muslim-other-unaffaliated",
    "href": "code/QDA.html#using-selected-for-protestant-catholic-mormon-christian-jewish-muslim-other-unaffaliated",
    "title": "QDA",
    "section": "",
    "text": "Code\n# reading in data\nreligion = pd.read_csv(\"../data/religion_full_currel.csv\")\n\n\n\n\n\n\n\nCode\n# visualizing imbalance\ny = religion['CURREL']\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# dropping refused \nreligion = religion[religion['CURREL'] != 900000]\n\n# Christian, non-christian, unaffiliated\ngrouping_map = {\n    1000: 'Protestant',\n    10000: 'Catholic',\n    20000: 'Mormon',\n    30000: 'Christian',\n    40001: 'Christian',\n    40002: 'Christian',\n    50000: 'Jewish',\n    60000: 'Muslim',\n    70000: 'Other Religion',\n    80000: 'Other Religion',\n    90001: 'Other Religion',\n    90002: 'Other Religion',\n    100000: 'Unaffiliated'\n}\n\nreligion['CURREL_NEW'] = religion['CURREL'].map(grouping_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# getting the x and y variables\nX_int = religion.drop(columns=['RELTRAD', 'CURREL_NEW'])\n\n# take currel\ny = religion['CURREL_NEW']\n\n# drop some rows for y\nprint(y.value_counts())\n\n# checking shapes\nprint(y.shape)\nprint(X_int.shape)\n\n\nCURREL_NEW\nProtestant        8723\nUnaffiliated      7355\nCatholic          4074\nOther Religion     963\nJewish             521\nMormon             362\nChristian          283\nMuslim             167\nName: count, dtype: int64\n(22448,)\n(22448, 100)\n\n\n\n\nCode\nselected_features = [0, 7, 8, 10, 11, 13, 14, 16, 17, 19, 21, 22, 24, 25, 26, 27, 29, 30, 32, 33, 35, 36, 37, 38, 44, 47, 50, 53, 54, 57, 60, 62, 63, 64, 65, 70, 71, 74, 75, 77, 80, 81, 82, 83, 85, 86, 88, 90, 91, 92]\n\nX_int = X_int.iloc[:, selected_features]\nprint(X_int.shape)\n\n\n(22448, 50)\n\n\n\n\n\n\n\nCode\n# scaling X value\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_int)\n\n# split data into test, train, validation\nX_tmp, X_test, y_tmp, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=6600)\nX_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=0.2, random_state=6600)\n\n# print\nprint(\"\\nTRAIN\")\nprint(X_train.shape)\nprint(y_train.shape)\n\nprint(\"\\nVALIDATION\")\nprint(X_val.shape)\nprint(y_val.shape)\n\nprint(\"\\nTEST\")\nprint(X_test.shape)\nprint(y_test.shape)\n\n\n\nTRAIN\n(14366, 50)\n(14366,)\n\nVALIDATION\n(3592, 50)\n(3592,)\n\nTEST\n(4490, 50)\n(4490,)\n\n\n\n\n\n\n\nCode\n# checking the imbalance\nsns.countplot(x=y)\nplt.xticks(rotation=45)\nplt.title(\"Class Distribution in CURREL\")\nplt.show()\n\n# adding re-sampling to deal with class imabalance\nsmote = SMOTE(random_state=6600)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# initiating parameters\nbest_val = 0\nopt_reg = None\nval_scores = {}\nreg_params = [0.0, 0.05, 0.1, 0.2, 0.5, 0.9]\n\nfor r in reg_params:\n    qda_model = QuadraticDiscriminantAnalysis(reg_param=0.2)\n    qda_model.fit(X_train_bal, y_train_bal)\n\n    # getting the predictions\n    y_val_pred = qda_model.predict(X_val)\n    val_score = accuracy_score(y_val, y_val_pred)\n    val_scores[r] = val_score\n\n    # updating best reg value\n    if val_score &gt; best_val:\n        best_val = val_score\n        opt_reg = r\n\nprint(\"\\nOptimal reg_param:\", opt_reg)\nprint(\"Validation accuracy:\", best_val)\n\n# plotting\nplt.plot(val_scores.keys(), val_scores.values(), marker='o')\nplt.title(\"Validation Accuracy vs reg_param\")\nplt.xlabel(\"reg_param\")\nplt.ylabel(\"Validation Accuracy\")\nplt.grid(True)\nplt.show()\n\n\n\nOptimal reg_param: 0.0\nValidation accuracy: 0.6620267260579065\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nopt_reg = 0.2\nqda_model = QuadraticDiscriminantAnalysis(reg_param=opt_reg) # iniating QDA\nqda_model.fit(X_train_bal, y_train_bal)\n\n# getting the predictions\ny_train_pred = qda_model.predict(X_train)\ny_test_pred = qda_model.predict(X_test)\ny_val_pred = qda_model.predict(X_val)\n\n\n\n\n\n\n\nCode\n# classification report\nprint(\"CLASSIFICATION REPORT:\")\nprint(classification_report(y_test, y_test_pred, zero_division=0))\nprint(accuracy_score(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# confusion matrix\nprint(\"CONFUSION MATRIX:\")\nprint(confusion_matrix(y_test, y_test_pred))\nprint(\"-----------------------\")\n\n# ROC curve\nprint(\"ROC CURVES:\")\nclasses = qda_model.classes_ # getting classes\ny_score = qda_model.predict_proba(X_test) # predictions\ny_onehot = label_binarize(y_test, classes=classes)\nfor i, label in enumerate(classes): # plotting ROC for all classes of digits\n    auc = roc_auc_score(y_onehot[:, i], y_score[:, i])\n    display = RocCurveDisplay.from_predictions( # ROC\n        y_true=y_onehot[:, i],\n        y_pred=y_score[:, i],\n        name=f\"Religion {label} vs the rest\",\n        color=\"darkorange\",\n        plot_chance_level=True,\n        despine=True,\n        )\n    _ = display.ax_.set(\n        xlabel=\"False Positive Rate\",\n        ylabel=\"True Positive Rate\"\n    )\nplt.show()\n\n\nCLASSIFICATION REPORT:\n                precision    recall  f1-score   support\n\n      Catholic       0.60      0.62      0.61       815\n     Christian       0.12      0.30      0.18        63\n        Jewish       0.26      0.60      0.36       107\n        Mormon       0.10      0.74      0.18        69\n        Muslim       0.40      0.49      0.44        37\nOther Religion       0.27      0.45      0.34       173\n    Protestant       0.85      0.55      0.66      1777\n  Unaffiliated       0.94      0.83      0.88      1449\n\n      accuracy                           0.65      4490\n     macro avg       0.44      0.57      0.46      4490\n  weighted avg       0.77      0.65      0.69      4490\n\n0.6478841870824054\n-----------------------\nCONFUSION MATRIX:\n[[ 502   36   55   71    2    4  142    3]\n [   6   19    7    7    0    8   15    1]\n [  13    6   64    2    2    7    4    9]\n [   6    2    1   51    0    2    6    1]\n [   2    5    3    2   18    4    2    1]\n [   3    4   14    1   14   77    1   59]\n [ 289   71   82  359    3    0  971    2]\n [  14    9   23    4    6  181    5 1207]]\n-----------------------\nROC CURVES:"
  },
  {
    "objectID": "code/QDA.html#final-results-summary",
    "href": "code/QDA.html#final-results-summary",
    "title": "QDA",
    "section": "",
    "text": "Full Currel: - overall accuracy: 0.67\n- weighted f-1: 0.70\n\nChristian vs Non vs Unaffaliated: - overall accuracy: 0.89\n- weighted f-1: 0.90\n\nMore Categories: - overall accuracy: 0.68\n- weighted f-1: 0.71\n\nFull Currel w Features: - overall accuracy: 0.60\n- weighted f-1: 0.66\n\nChristian vs Non vs Unaffaliated w Features: - overall accuracy: 0.91\n- weighted f-1: 0.91\n\nMore Categories w Features: - overall accuracy: 0.65\n- weighted f-1: 0.69"
  }
]